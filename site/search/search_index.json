{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Smart Pixels (ML) Smart Pixels is a project focused on implementing machine learning models directly on silicon pixels or future detectors to enhance the inference of charged particle track parameters. We here use an MDN (Mixture Density Network) with one Gaussian. This model predicts the mean value of the target variable and the associated uncertainty to that prediction, with NLL (Negative Log-likelihood Loss) as the loss function to minimize. Table of Contents Getting Started Installation Usage Dataset Structure Data Visualization Data Generator Testing Model Architecture Model Evaluation Getting Started Installation Clone the repository: git clone https://github.com/smart-pix/smart-pixels-ml.git cd smart-pixels-ml pip install -r requirements.txt Testing successful installation by running python test_run.py` Usage For the usage guide refer to Usage Guide Dataset Download the simulated data from: zenodo and PixelAV Structure Input Data (dataX) : Consists of 2D images representing the charge deposited by a particle, with each channel showing a different time slice of the particle\u2019s passage. You can choose to work with either 2 time slices (reduced) or the full 20 time slices. These correspond to input shapes of (13, 21, 2) and (13, 21, 20) respectively. Labels (dataY) : Four target variables are chosen as the labls viz. local $x$, local $y$, $\\alpha$ angle and $\\beta$ angle, associated with the particle trajectory. Data Visualization For visualization of the how the input data looks like, we have to define the path towards the dataX and optionally to the labels as import pandas as pd import matplotlib.pyplot as plt dataX = pd.read_parquet(\"path/to/recon3D_data_file\") labels_df = pd.read_parquet(\"path/to/labels_data_file\") reshaped_dataX = dataX.values.reshape((len(dataX), 20, 13, 21)) print(labels_df.iloc[0]) plt.imshow(reshaped_dataX[0, 0, :, :], cmap='coolwarm') # first time-step plt.show() plt.imshow(reshaped_dataX[0, -1, :, :], cmap='coolwarm') # last time-step plt.show() Data Generator Due to the large size of the dataset, the entire dataset can not be loaded into RAM. Hence, we use data generators to load the dataset on the fly during training with inbuilt quantization, standardization, shuffling, etc. Refer to data generator for more details. Testing To test that everything is working fine try running the simple test_run.py file as python test_run.py Model Architecture The core model architecture is defined in model.py , which provides the baseline MDN architecture with quantized neural network layers. We use the Negative Log-Likelihood (NLL) as the loss function implemented in loss.py . A good reading about it can be found here Refer to Model and Loss for more details. As an example, to implement the model with 2 time slices: from model import * model=CreateModel((13,21,2),n_filters=5,pool_size=3) model.summary() This generates a model with the following architecture: | Type | Output Shape | Parameters | |----------------------|---------------------|------------| | InputLayer | (None, 13, 21, 2) | 0 | | QSeparableConv2D | (None, 11, 19, 5) | 33 | | QActivation | (None, 11, 19, 5) | 0 | | QConv2D | (None, 11, 19, 5) | 30 | | QActivation | (None, 11, 19, 5) | 0 | | AveragePooling2D | (None, 3, 6, 5) | 0 | | QActivation | (None, 3, 6, 5) | 0 | | Flatten | (None, 90) | 0 | | QDense | (None, 16) | 1456 | | QActivation | (None, 16) | 0 | | QDense | (None, 16) | 272 | | QActivation | (None, 16) | 0 | | QDense | (None, 14) | 238 | Model Evaluation Refer to Evaluate for more details","title":"Home"},{"location":"#smart-pixels-ml","text":"Smart Pixels is a project focused on implementing machine learning models directly on silicon pixels or future detectors to enhance the inference of charged particle track parameters. We here use an MDN (Mixture Density Network) with one Gaussian. This model predicts the mean value of the target variable and the associated uncertainty to that prediction, with NLL (Negative Log-likelihood Loss) as the loss function to minimize.","title":"Smart Pixels (ML)"},{"location":"#table-of-contents","text":"Getting Started Installation Usage Dataset Structure Data Visualization Data Generator Testing Model Architecture Model Evaluation","title":"Table of Contents"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#installation","text":"Clone the repository: git clone https://github.com/smart-pix/smart-pixels-ml.git cd smart-pixels-ml pip install -r requirements.txt Testing successful installation by running python test_run.py`","title":"Installation"},{"location":"#usage","text":"For the usage guide refer to Usage Guide","title":"Usage"},{"location":"#dataset","text":"Download the simulated data from: zenodo and PixelAV","title":"Dataset"},{"location":"#structure","text":"Input Data (dataX) : Consists of 2D images representing the charge deposited by a particle, with each channel showing a different time slice of the particle\u2019s passage. You can choose to work with either 2 time slices (reduced) or the full 20 time slices. These correspond to input shapes of (13, 21, 2) and (13, 21, 20) respectively. Labels (dataY) : Four target variables are chosen as the labls viz. local $x$, local $y$, $\\alpha$ angle and $\\beta$ angle, associated with the particle trajectory.","title":"Structure"},{"location":"#data-visualization","text":"For visualization of the how the input data looks like, we have to define the path towards the dataX and optionally to the labels as import pandas as pd import matplotlib.pyplot as plt dataX = pd.read_parquet(\"path/to/recon3D_data_file\") labels_df = pd.read_parquet(\"path/to/labels_data_file\") reshaped_dataX = dataX.values.reshape((len(dataX), 20, 13, 21)) print(labels_df.iloc[0]) plt.imshow(reshaped_dataX[0, 0, :, :], cmap='coolwarm') # first time-step plt.show() plt.imshow(reshaped_dataX[0, -1, :, :], cmap='coolwarm') # last time-step plt.show()","title":"Data Visualization"},{"location":"#data-generator","text":"Due to the large size of the dataset, the entire dataset can not be loaded into RAM. Hence, we use data generators to load the dataset on the fly during training with inbuilt quantization, standardization, shuffling, etc. Refer to data generator for more details.","title":"Data Generator"},{"location":"#testing","text":"To test that everything is working fine try running the simple test_run.py file as python test_run.py","title":"Testing"},{"location":"#model-architecture","text":"The core model architecture is defined in model.py , which provides the baseline MDN architecture with quantized neural network layers. We use the Negative Log-Likelihood (NLL) as the loss function implemented in loss.py . A good reading about it can be found here Refer to Model and Loss for more details. As an example, to implement the model with 2 time slices: from model import * model=CreateModel((13,21,2),n_filters=5,pool_size=3) model.summary() This generates a model with the following architecture: | Type | Output Shape | Parameters | |----------------------|---------------------|------------| | InputLayer | (None, 13, 21, 2) | 0 | | QSeparableConv2D | (None, 11, 19, 5) | 33 | | QActivation | (None, 11, 19, 5) | 0 | | QConv2D | (None, 11, 19, 5) | 30 | | QActivation | (None, 11, 19, 5) | 0 | | AveragePooling2D | (None, 3, 6, 5) | 0 | | QActivation | (None, 3, 6, 5) | 0 | | Flatten | (None, 90) | 0 | | QDense | (None, 16) | 1456 | | QActivation | (None, 16) | 0 | | QDense | (None, 16) | 272 | | QActivation | (None, 16) | 0 | | QDense | (None, 14) | 238 |","title":"Model Architecture"},{"location":"#model-evaluation","text":"Refer to Evaluate for more details","title":"Model Evaluation"},{"location":"plot/","text":"Plots Add things about the plots here","title":"Visualization"},{"location":"plot/#plots","text":"Add things about the plots here","title":"Plots"},{"location":"testing/","text":"Testing Module This module contains unit tests for the Smart Pixels ML project. Functions test_run() Arguments :","title":"Testing"},{"location":"testing/#testing-module","text":"This module contains unit tests for the Smart Pixels ML project.","title":"Testing Module"},{"location":"testing/#functions","text":"","title":"Functions"},{"location":"testing/#test_run","text":"Arguments :","title":"test_run()"},{"location":"usage/","text":"Usage Guide This is a guide for using the Smart Pixels ML project to train and evaluate the models on the simulated data. 1. Installation Refer to Readme for installation instructions. 2. Data Collection Download the simulated data from zenodo and PixelAV Add other links here Ensure the two directories Data and Labels are present. 3. Data Preparation Define the paths to the data and labels directories (look at utils for more details) Configure datagenerator parameters (look at data_generator for more details) Create training and validation datagenerators 4. Model Creation Define the model architecture and compile. Also look at the summary of the model to ensure it is correct. Look at model for more details on how to do that. For loss function see loss . 5. Model Training If everything is set up correctly, the training should start and run seamlessly. For example: model.fit( x=training_generator, validation_data=validation_generator, epochs=200, verbose=1) After training, check the loss and accuracy of the model. And save the model weights. 6. Model Evaluation Initiate the model and Load the weights as model=CreateModel((13,21,2),n_filters=5,pool_size=3) model.load_weights(\"model_weights.h5\") And then evaluate the model as done in evaluate.py or in evaluate.py 7. Model Prediction Look at predict 8. Add Additional Instructions Here are some additional instructions","title":"Usage"},{"location":"usage/#usage-guide","text":"This is a guide for using the Smart Pixels ML project to train and evaluate the models on the simulated data.","title":"Usage Guide"},{"location":"usage/#1-installation","text":"Refer to Readme for installation instructions.","title":"1. Installation"},{"location":"usage/#2-data-collection","text":"Download the simulated data from zenodo and PixelAV Add other links here Ensure the two directories Data and Labels are present.","title":"2. Data Collection"},{"location":"usage/#3-data-preparation","text":"Define the paths to the data and labels directories (look at utils for more details) Configure datagenerator parameters (look at data_generator for more details) Create training and validation datagenerators","title":"3. Data Preparation"},{"location":"usage/#4-model-creation","text":"Define the model architecture and compile. Also look at the summary of the model to ensure it is correct. Look at model for more details on how to do that. For loss function see loss .","title":"4. Model Creation"},{"location":"usage/#5-model-training","text":"If everything is set up correctly, the training should start and run seamlessly. For example: model.fit( x=training_generator, validation_data=validation_generator, epochs=200, verbose=1) After training, check the loss and accuracy of the model. And save the model weights.","title":"5. Model Training"},{"location":"usage/#6-model-evaluation","text":"Initiate the model and Load the weights as model=CreateModel((13,21,2),n_filters=5,pool_size=3) model.load_weights(\"model_weights.h5\") And then evaluate the model as done in evaluate.py or in evaluate.py","title":"6. Model Evaluation"},{"location":"usage/#7-model-prediction","text":"Look at predict","title":"7. Model Prediction"},{"location":"usage/#8-add-additional-instructions","text":"Here are some additional instructions","title":"8. Add Additional Instructions"},{"location":"api/data_generator/","text":"datagenerator module This module contains the OptimizedDataGenerator class, which generates batches of data for training and validation during model training. This datagenerator handles the loading and processing of the data, including shuffling, standardization, and quantization of the data. It does by pre-processing the data and saving it as TFRecord files and then loading the batches on the fly during training. Methods __init__(...) Initialize the OptimizedDataGenerator class with the specified parameters to configure the data generator for preprocessing and batching. Arguments Described in the comments of the __init__ method of the OptimizedDataGenerator.py file. Example Usage Initializing the Data Generators training_generator = OptimizedDataGenerator( data_directory_path = \"path/to/data/\", labels_directory_path = \"path/to/labels/\", is_directory_recursive = False, file_type = \"parquet\", data_format = \"3D\", batch_size = val_batch_size, file_count = val_file_size, to_standardize= True, include_y_local= False, labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'], input_shape = (2,13,21), # (20,13,21), transpose = (0,2,3,1), shuffle = False, files_from_end=True, tfrecords_dir = \"path/to/tfrecords/\", use_time_stamps = [0, 19], #-1 max_workers = 1, # Don't make this too large (will use up all RAM) seed = 10, quantize = True # Quantization ON ) Loading the Data Generators Already generated TFRecords can be reused by setting load_from_tfrecords_dir as training_generator = OptimizedDataGenerator( load_from_tfrecords_dir = \"path/to/tfrecords/\", shuffle = True, seed = 13, quantize = True ) The same goes for the validation generator . Using the Data Generators The data generators can be directly passed to the fit method of a Keras model. history = model.fit( x=training_generator, validation_data=validation_generator, #callbacks=[es, mcp, csv_logger], epochs=1000, shuffle=False, verbose=1 )","title":"Data Generator"},{"location":"api/data_generator/#datagenerator-module","text":"This module contains the OptimizedDataGenerator class, which generates batches of data for training and validation during model training. This datagenerator handles the loading and processing of the data, including shuffling, standardization, and quantization of the data. It does by pre-processing the data and saving it as TFRecord files and then loading the batches on the fly during training.","title":"datagenerator module"},{"location":"api/data_generator/#methods","text":"","title":"Methods"},{"location":"api/data_generator/#__init__","text":"Initialize the OptimizedDataGenerator class with the specified parameters to configure the data generator for preprocessing and batching.","title":"__init__(...)"},{"location":"api/data_generator/#arguments","text":"Described in the comments of the __init__ method of the OptimizedDataGenerator.py file.","title":"Arguments"},{"location":"api/data_generator/#example-usage","text":"","title":"Example Usage"},{"location":"api/data_generator/#initializing-the-data-generators","text":"training_generator = OptimizedDataGenerator( data_directory_path = \"path/to/data/\", labels_directory_path = \"path/to/labels/\", is_directory_recursive = False, file_type = \"parquet\", data_format = \"3D\", batch_size = val_batch_size, file_count = val_file_size, to_standardize= True, include_y_local= False, labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'], input_shape = (2,13,21), # (20,13,21), transpose = (0,2,3,1), shuffle = False, files_from_end=True, tfrecords_dir = \"path/to/tfrecords/\", use_time_stamps = [0, 19], #-1 max_workers = 1, # Don't make this too large (will use up all RAM) seed = 10, quantize = True # Quantization ON )","title":"Initializing the Data Generators"},{"location":"api/data_generator/#loading-the-data-generators","text":"Already generated TFRecords can be reused by setting load_from_tfrecords_dir as training_generator = OptimizedDataGenerator( load_from_tfrecords_dir = \"path/to/tfrecords/\", shuffle = True, seed = 13, quantize = True ) The same goes for the validation generator .","title":"Loading the Data Generators"},{"location":"api/data_generator/#using-the-data-generators","text":"The data generators can be directly passed to the fit method of a Keras model. history = model.fit( x=training_generator, validation_data=validation_generator, #callbacks=[es, mcp, csv_logger], epochs=1000, shuffle=False, verbose=1 )","title":"Using the Data Generators"},{"location":"api/evaluate/","text":"Evaluate Module The evaluate.py file defines the evaluation function for the model. Functions: evaluate(config) Arguments : config (dict): Configuration dictionary containing the parameters for evaluation. Example Usage: config = { \"weightsPath\": \"path/to/weights.hdf5\", \"outFileName\": \"path/to/evaluation_results.csv\", \"data_directory_path\": \"path/to/data/\", \"labels_directory_path\": \"path/to/labels/\", \"n_filters\": 5, \"pool_size\": 3, \"val_batch_size\": 500, \"val_file_size\": 10 } evaluate(config)","title":"Evaluate"},{"location":"api/evaluate/#evaluate-module","text":"The evaluate.py file defines the evaluation function for the model.","title":"Evaluate Module"},{"location":"api/evaluate/#functions-evaluateconfig","text":"Arguments : config (dict): Configuration dictionary containing the parameters for evaluation.","title":"Functions: evaluate(config)"},{"location":"api/evaluate/#example-usage","text":"config = { \"weightsPath\": \"path/to/weights.hdf5\", \"outFileName\": \"path/to/evaluation_results.csv\", \"data_directory_path\": \"path/to/data/\", \"labels_directory_path\": \"path/to/labels/\", \"n_filters\": 5, \"pool_size\": 3, \"val_batch_size\": 500, \"val_file_size\": 10 } evaluate(config)","title":"Example Usage:"},{"location":"api/loss/","text":"Loss Module This module uses a Mixture Density Network(MDN) and hence a negative log-likelihood loss function. It uses TensorFlow and TensorFlow Probability for the loss computation. Loss Function custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale=512) Calculates the Negative Log-Likelihood (NLL) for a batch of data using the model's predicted parameters. Brief Description The model parameters are the mean and the lower triangular part of the covariance matrix. loss function is vectorized with batches. Arguments y (tf.Tensor): The target data, shape (batch_size, 4). p_base (tf.Tensor): The predicted parameters, shape (batch_size, 16). minval (float): The minimum value for the likelihood, default 1e-9. maxval (float): The maximum value for the likelihood, default 1e9. scale (float): . Returns tf.Tensor : Negative Log-Likelihood (NLL) for the given batch., shape (batch_size,). Example Usage import tensorflow as tf from tensorflow.keras.optimizers import Adam from loss import custom_loss from models import CreateModel model = CreateModel((13, 21, 2), n_filters=5, pool_size=3) model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)","title":"Loss"},{"location":"api/loss/#loss-module","text":"This module uses a Mixture Density Network(MDN) and hence a negative log-likelihood loss function. It uses TensorFlow and TensorFlow Probability for the loss computation.","title":"Loss Module"},{"location":"api/loss/#loss-function","text":"","title":"Loss Function"},{"location":"api/loss/#custom_lossy-p_base-minval1e-9-maxval1e9-scale512","text":"Calculates the Negative Log-Likelihood (NLL) for a batch of data using the model's predicted parameters.","title":"custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale=512)"},{"location":"api/loss/#brief-description","text":"The model parameters are the mean and the lower triangular part of the covariance matrix. loss function is vectorized with batches.","title":"Brief Description"},{"location":"api/loss/#arguments","text":"y (tf.Tensor): The target data, shape (batch_size, 4). p_base (tf.Tensor): The predicted parameters, shape (batch_size, 16). minval (float): The minimum value for the likelihood, default 1e-9. maxval (float): The maximum value for the likelihood, default 1e9. scale (float): .","title":"Arguments"},{"location":"api/loss/#returns","text":"tf.Tensor : Negative Log-Likelihood (NLL) for the given batch., shape (batch_size,).","title":"Returns"},{"location":"api/loss/#example-usage","text":"import tensorflow as tf from tensorflow.keras.optimizers import Adam from loss import custom_loss from models import CreateModel model = CreateModel((13, 21, 2), n_filters=5, pool_size=3) model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)","title":"Example Usage"},{"location":"api/models/","text":"Models Module The models.py file defines Mixture Density Network (MDN) network with a 4D Multivariate Normal Distribution neural network architecture using quantized layers. The implementation uses QKeras to quantize the weights and activations of the network. Functions CreateModel(shape, n_filters, pool_size) Creates a quantized neural network model for regression task with quantized layers and activations as in Model . The model has 14 output nodes with 4 being the target variables and the rest 10 being the co-variances. Arguments : shape (tuple): Input shape (e.g., (13, 21, 2) / (13, 21, 20) ). n_filters (int): Number of filters for the convolutional layers. pool_size (int): Size of the pool for the pooling layer. Returns : keras.Model : A compiled Keras model instance. Example : ```python from models import CreateModel model = CreateModel((13, 21, 2), n_filters=5, pool_size=3) model.summary() Additional Helper Functions conv_network(var, n_filters=5, kernel_size=3) Defines the convolutional network block, with quantized layers and activations. Arguments : var (InputLayer: tf.Tensor) : Input tensor. n_filters (int) : Number of filters. kernel_size (int) : Kernel size. Returns : tf.Tensor : Output tensor. var_network(var, hidden=10, output=2) Defines the dense network block, with quantized layers and activations. Arguments : var (InputLayer: tf.Tensor) : Input tensor. hidden (int) : Number of hidden units. output (int) : Number of output units. Returns : tf.Tensor : Output tensor.","title":"Models"},{"location":"api/models/#models-module","text":"The models.py file defines Mixture Density Network (MDN) network with a 4D Multivariate Normal Distribution neural network architecture using quantized layers. The implementation uses QKeras to quantize the weights and activations of the network.","title":"Models Module"},{"location":"api/models/#functions","text":"","title":"Functions"},{"location":"api/models/#createmodelshape-n_filters-pool_size","text":"Creates a quantized neural network model for regression task with quantized layers and activations as in Model . The model has 14 output nodes with 4 being the target variables and the rest 10 being the co-variances. Arguments : shape (tuple): Input shape (e.g., (13, 21, 2) / (13, 21, 20) ). n_filters (int): Number of filters for the convolutional layers. pool_size (int): Size of the pool for the pooling layer. Returns : keras.Model : A compiled Keras model instance. Example : ```python from models import CreateModel model = CreateModel((13, 21, 2), n_filters=5, pool_size=3) model.summary()","title":"CreateModel(shape, n_filters, pool_size)"},{"location":"api/models/#additional-helper-functions","text":"","title":"Additional Helper Functions"},{"location":"api/models/#conv_networkvar-n_filters5-kernel_size3","text":"Defines the convolutional network block, with quantized layers and activations. Arguments : var (InputLayer: tf.Tensor) : Input tensor. n_filters (int) : Number of filters. kernel_size (int) : Kernel size. Returns : tf.Tensor : Output tensor.","title":"conv_network(var, n_filters=5, kernel_size=3)"},{"location":"api/models/#var_networkvar-hidden10-output2","text":"Defines the dense network block, with quantized layers and activations. Arguments : var (InputLayer: tf.Tensor) : Input tensor. hidden (int) : Number of hidden units. output (int) : Number of output units. Returns : tf.Tensor : Output tensor.","title":"var_network(var, hidden=10, output=2)"},{"location":"api/plotting/","text":"Plot Module Add Plot results here","title":"Plotting"},{"location":"api/plotting/#plot-module","text":"Add Plot results here","title":"Plot Module"},{"location":"api/utils/","text":"Utils Module This module contains utility functions to manage file operations and GPU configurations. Functions safe_remove_directory(directory_path) Safely removes a directory if it exists. Arguments : directory_path (str): Path to the directory to be removed. Example : ```python from utils import safe_remove_directory safe_remove_directory(\"./temp_folder\") check_GPU() Checks for available GPUs and sets memory growth to prevent allocation issues. Arguments : None. Return Prints GPU information. Example : ```python from utils import safe_remove_directory safe_remove_directory(\"./temp_folder\")","title":"Utils"},{"location":"api/utils/#utils-module","text":"This module contains utility functions to manage file operations and GPU configurations.","title":"Utils Module"},{"location":"api/utils/#functions","text":"","title":"Functions"},{"location":"api/utils/#safe_remove_directorydirectory_path","text":"Safely removes a directory if it exists. Arguments : directory_path (str): Path to the directory to be removed. Example : ```python from utils import safe_remove_directory safe_remove_directory(\"./temp_folder\")","title":"safe_remove_directory(directory_path)"},{"location":"api/utils/#check_gpu","text":"Checks for available GPUs and sets memory growth to prevent allocation issues. Arguments : None. Return Prints GPU information. Example : ```python from utils import safe_remove_directory safe_remove_directory(\"./temp_folder\")","title":"check_GPU()"}]}