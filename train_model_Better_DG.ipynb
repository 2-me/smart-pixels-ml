{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:19:28.623939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-07 01:19:28.624045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-07 01:19:28.657384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-07 01:19:28.930454: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-07 01:19:39.216322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9\n",
    "\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08697ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/das214/SmartPix/Datagenerator_debug\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('SmartPix/data_generator')\n",
    "os.chdir('SmartPix/Datagenerator_debug')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataprep import *\n",
    "# from OptimizedDataGeneratorNew import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizedDataGeneratorNew.py\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from qkeras import quantized_bits\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "# custom quantizer\n",
    "\n",
    "# @tf.function\n",
    "def QKeras_data_prep_quantizer(data, bits=4, int_bits=0, alpha=1):\n",
    "    \"\"\"\n",
    "    Applies QKeras quantization.\n",
    "    Args:\n",
    "        data (tf.Tensor): Input data (tf.Tensor).\n",
    "        bits (int): Number of bits for quantization.\n",
    "        int_bits (int): Number of integer bits.\n",
    "        alpha (float): (don't change)\n",
    "    Returns::\n",
    "        tf.Tensor: Quantized data (tf.Tensor).\n",
    "    \"\"\"\n",
    "    quantizer = quantized_bits(bits, int_bits, alpha=alpha)\n",
    "    return quantizer(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, \n",
    "            dataset_base_dir: str = \"./\",\n",
    "            batch_size: int = 32,\n",
    "            optimize_batch_size: bool = False,\n",
    "            file_count = None,\n",
    "            labels_list: Union[List,str] = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "            to_standardize: bool = False,\n",
    "            input_shape: Tuple = (13,21),\n",
    "            transpose = None,\n",
    "            files_from_end = False,\n",
    "            shuffle=False,\n",
    "\n",
    "            # Added in Optimized datagenerators \n",
    "            load_from_tfrecords_dir: str = None,\n",
    "            tfrecords_dir: str = None,\n",
    "            use_time_stamps = -1,\n",
    "            seed: int = None,\n",
    "            quantize: bool = False,\n",
    "            max_workers: int = 1,\n",
    "            label_scale_pctl: float = 99,\n",
    "            norm_pos_pctl: float = 99.7,\n",
    "            norm_neg_pctl: float = 99.7,\n",
    "            tail_tol: float = 0.75,\n",
    "            **kwargs,\n",
    "            ):\n",
    "        super().__init__() \n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle:\n",
    "            self.seed = seed if seed is not None else 13\n",
    "            self.rng = np.random.default_rng(seed = self.seed)\n",
    "        \n",
    "        # If data is already prepared load -> load that data and use\n",
    "        if load_from_tfrecords_dir is not None:\n",
    "            # LOADER MODE\n",
    "            self.file_offsets = [None]\n",
    "            if not os.path.isdir(load_from_tfrecords_dir):\n",
    "                raise ValueError(f\"Directory {load_from_tfrecords_dir} does not exist.\")\n",
    "            \n",
    "            self.tfrecords_dir = load_from_tfrecords_dir\n",
    "            metadata_file_path = os.path.join(self.tfrecords_dir, \"metadata.json\")\n",
    "            self.load_metadata(metadata_file_path)\n",
    "            \n",
    "        else:\n",
    "            # CREATOR MODE\n",
    "            n_time, height, width = input_shape\n",
    "            \n",
    "            if use_time_stamps == -1:\n",
    "                use_time_stamps = list(np.arange(0,20))\n",
    "            assert len(use_time_stamps) == n_time, f\"Expected {n_time} time steps, got {len(use_time_stamps)}\"\n",
    "    \n",
    "            len_xy = height * width\n",
    "            col_indices = [\n",
    "                np.arange(t * len_xy, (t + 1) * len_xy).astype(str)\n",
    "                for t in use_time_stamps\n",
    "            ]\n",
    "            self.recon_cols = np.concatenate(col_indices).tolist()\n",
    "    \n",
    "            self.max_workers = max_workers\n",
    "            self.label_scale_pctl = label_scale_pctl\n",
    "            self.norm_pos_pctl = norm_pos_pctl\n",
    "            self.norm_neg_pctl = norm_neg_pctl\n",
    "\n",
    "            \n",
    "            self.files = sorted(glob.glob(os.path.join(dataset_base_dir, \"part.*.parquet\"), recursive=False))\n",
    "    \n",
    "            if file_count != None:\n",
    "                if not files_from_end:\n",
    "                    self.files = self.files[:file_count]\n",
    "                else:\n",
    "                    self.files = self.files[-file_count:]\n",
    "    \n",
    "            self.file_offsets = [0]\n",
    "            self.dataset_mean = None\n",
    "            self.dataset_std = None\n",
    "            self.norm_factor_pos = None  \n",
    "            self.norm_factor_neg = None\n",
    "            self.labels_scale = None\n",
    "\n",
    "            self.labels_list = labels_list\n",
    "            self.input_shape = input_shape\n",
    "            self.transpose = transpose\n",
    "            self.to_standardize = to_standardize\n",
    "\n",
    "           \n",
    "\n",
    "            self.process_file_parallel()\n",
    "            \n",
    "            \n",
    "            if optimize_batch_size:\n",
    "                original_bs = batch_size\n",
    "                new_bs, residual = self.get_best_batch_size(self.file_offsets, original_bs)\n",
    "                \n",
    "                if new_bs != original_bs:\n",
    "                    print(f\"Batch size optimized from {original_bs} to {new_bs} \"\n",
    "                        f\"to minimize final batch (residual: {residual} rows).\")\n",
    "                \n",
    "                self.batch_size = new_bs\n",
    "            else:\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "            self.batch_metadata = self.build_batch_metadata(\n",
    "                batch_size=self.batch_size, \n",
    "                file_offsets=self.file_offsets, \n",
    "                tail_tol=tail_tol\n",
    "            )\n",
    "    \n",
    "            self.current_file_index = None\n",
    "            self.current_dataframes = None\n",
    "    \n",
    "            if tfrecords_dir is None:\n",
    "                raise ValueError(f\"tfrecords_dir is None\")\n",
    "            utils.safe_remove_directory(tfrecords_dir)\n",
    "                \n",
    "            self.tfrecords_dir = tfrecords_dir    \n",
    "            os.makedirs(self.tfrecords_dir, exist_ok=True)\n",
    "            self.save_batches_sequentially()\n",
    "            del self.current_dataframes \n",
    "            \n",
    "            metadata_file_path = os.path.join(self.tfrecords_dir, \"metadata.json\")\n",
    "            self.save_metadata(metadata_file_path)\n",
    "            \n",
    "        self.tfrecord_filenames = np.sort(np.array(tf.io.gfile.glob(os.path.join(self.tfrecords_dir, \"*.tfrecord\"))))\n",
    "        self.quantize = quantize\n",
    "        self.epoch_count = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def save_metadata(self, metadata_file_path:str):\n",
    "        \"\"\"\n",
    "        Saves the metadata of the dataset to a JSON file.\n",
    "        Args:\n",
    "            metadata_file_path (str): Path to save the metadata file.\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            # Key configurations\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"input_shape\": self.input_shape,\n",
    "            \"recon_cols\": self.recon_cols,\n",
    "            \"labels_list\": self.labels_list,\n",
    "            \"to_standardize\": self.to_standardize,\n",
    "            \"transpose\": self.transpose,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"seed\": self.seed,\n",
    "            \"label_scale_pctl\": self.label_scale_pctl,\n",
    "            \"norm_pos_pctl\": self.norm_pos_pctl,\n",
    "            \"norm_neg_pctl\": self.norm_neg_pctl,\n",
    "            \"tail_tol\": self.tail_tol,\n",
    "            \n",
    "            # Calculated statistics\n",
    "            \"dataset_mean\": self.dataset_mean.tolist() if self.dataset_mean is not None else None,\n",
    "            \"dataset_std\": self.dataset_std.tolist() if self.dataset_std is not None else None,\n",
    "            \"dataset_min\": self.dataset_min if self.dataset_min is not None else None,\n",
    "            \"dataset_max\": self.dataset_max if self.dataset_max is not None else None,\n",
    "            \"norm_factor_pos\": self.norm_factor_pos,\n",
    "            \"norm_factor_neg\": self.norm_factor_neg,\n",
    "            \"labels_scale\": self.labels_scale.tolist() if self.labels_scale is not None else None,\n",
    "            \n",
    "            # Full batch plan\n",
    "            \"batch_metadata\": self.batch_metadata\n",
    "            \n",
    "            \n",
    "        }\n",
    "        with open(metadata_file_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        print(f\"Metadata saved successfully ast {metadata_file_path}\")\n",
    "        \n",
    "    def load_metadata(self, metadata_file_path:str):\n",
    "        \"\"\"\n",
    "        Loads the metadata of the dataset from a JSON file.\n",
    "        Args:\n",
    "            metadata_file_path (str): Path to the metadata file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(metadata_file_path):\n",
    "            raise FileNotFoundError(f\"Metadata file {metadata_file_path} does not exist.\\n\"\n",
    "                                    \"Cannot initialiize genrator in load mode.\")\n",
    "        print(f\"Loading metadata from {metadata_file_path}\")\n",
    "        with open(metadata_file_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Key configurations\n",
    "        self.batch_size = metadata['batch_size']\n",
    "        self.input_shape = tuple(metadata['input_shape'])\n",
    "        self.recon_cols = metadata['recon_cols']\n",
    "        self.labels_list = metadata['labels_list']\n",
    "        self.to_standardize = metadata['to_standardize']\n",
    "        self.label_scale_pctl = metadata['label_scale_pctl']\n",
    "        self.norm_pos_pctl = metadata['norm_pos_pctl']\n",
    "        self.norm_neg_pctl = metadata['norm_neg_pctl']\n",
    "        self.tail_tol = metadata['tail_tol']\n",
    "        \n",
    "        # Calculated statistics\n",
    "        self.dataset_mean = np.array(metadata['dataset_mean'])\n",
    "        self.dataset_std = np.array(metadata['dataset_std'])\n",
    "        self.dataset_min = metadata['dataset_min']\n",
    "        self.dataset_max = metadata['dataset_max']\n",
    "        self.norm_factor_pos = metadata['norm_factor_pos']\n",
    "        self.norm_factor_neg = metadata['norm_factor_neg']\n",
    "        self.labels_scale = np.array(metadata['labels_scale'])\n",
    "\n",
    "        # Full batch plan\n",
    "        self.batch_metadata = metadata['batch_metadata']\n",
    "        \n",
    "        \n",
    "        # Optional parameters\n",
    "        self.shuffle = metadata.get('shuffle', False)\n",
    "        self.seed = metadata.get('seed', 13)\n",
    "        self.transpose = metadata.get('transpose', None)\n",
    "        if self.shuffle:\n",
    "            self.rng = np.random.default_rng(seed=self.seed)\n",
    "            \n",
    "                \n",
    "        \n",
    "            \n",
    "\n",
    "    def process_file_parallel(self):\n",
    "        file_infos = [(afile, \n",
    "                    self.recon_cols, self.labels_list, \n",
    "                    self.label_scale_pctl, self.norm_pos_pctl, self.norm_neg_pctl) \n",
    "                    for afile in self.files\n",
    "                    ]\n",
    "        results = []\n",
    "        with ProcessPoolExecutor(self.max_workers) as executor:\n",
    "            futures = [executor.submit(self._process_file_single, file_info) for file_info in file_infos]\n",
    "            for future in tqdm(as_completed(futures), total=len(file_infos), desc=\"Processing Files...\"):\n",
    "                results.append(future.result())\n",
    "\n",
    "        for amean, avariance, amin, amax, num_rows, labels_scale, pos_scale, neg_scale in results:\n",
    "            self.file_offsets.append(self.file_offsets[-1] + num_rows)\n",
    "\n",
    "            if self.dataset_mean is None:\n",
    "                self.dataset_max = amax\n",
    "                self.dataset_min = amin\n",
    "                self.dataset_mean = amean\n",
    "                self.dataset_std = avariance\n",
    "            else:\n",
    "                self.dataset_max = max(self.dataset_max, amax)\n",
    "                self.dataset_min = min(self.dataset_min, amin)\n",
    "                self.dataset_mean += amean\n",
    "                self.dataset_std += avariance\n",
    "            \n",
    "            if self.labels_scale is None:\n",
    "                self.labels_scale = labels_scale\n",
    "            else:\n",
    "                self.labels_scale = np.maximum(self.labels_scale, labels_scale)\n",
    "\n",
    "            self.norm_factor_pos = (pos_scale if self.norm_factor_pos is None\n",
    "                                    else max(self.norm_factor_pos, pos_scale))\n",
    "            self.norm_factor_neg = (neg_scale if self.norm_factor_neg is None\n",
    "                                    else max(self.norm_factor_neg, neg_scale))\n",
    "\n",
    "        self.dataset_mean = self.dataset_mean / len(self.files)\n",
    "        self.dataset_std = np.sqrt(self.dataset_std / len(self.files)) \n",
    "            \n",
    "        self.file_offsets = np.array(self.file_offsets)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_file_single(file_info):\n",
    "        afile, recon_cols, labels_list, label_scale_pctl, norm_pos_pctl, norm_neg_pctl = file_info\n",
    "\n",
    "        df = pd.read_parquet(afile, columns=recon_cols + labels_list).reset_index(drop=True)\n",
    "        x = df[recon_cols].values\n",
    "\n",
    "        nonzeros = abs(x) > 0\n",
    "        x[nonzeros] = np.sign(x[nonzeros]) * np.log1p(abs(x[nonzeros])) / math.log(2)\n",
    "        amean, avariance = np.mean(x[nonzeros], keepdims=True), np.var(x[nonzeros], keepdims=True) + 1e-10\n",
    "        centered = np.zeros_like(x)\n",
    "        centered[nonzeros] = (x[nonzeros] - amean) / np.sqrt(avariance)\n",
    "        amin, amax = np.min(centered), np.max(centered)\n",
    "\n",
    "        pos_vals = np.abs(centered[centered  > 0])\n",
    "        neg_vals = np.abs(centered[centered  < 0])\n",
    "\n",
    "        pos_scale = (np.percentile(pos_vals, norm_pos_pctl)\n",
    "                    if pos_vals.size else 1.0)\n",
    "        neg_scale = (np.percentile(neg_vals, norm_neg_pctl)\n",
    "                    if neg_vals.size else 1.0)\n",
    "\n",
    "        len_adf = len(df)\n",
    "\n",
    "        labels_values = df[labels_list].values\n",
    "        labels_scale = np.percentile(np.abs(labels_values), label_scale_pctl, axis=0)\n",
    "\n",
    "        del df\n",
    "        gc.collect()\n",
    "        \n",
    "        return amean, avariance, amin, amax, len_adf, labels_scale, pos_scale, neg_scale\n",
    "\n",
    "    def standardize(self, x):\n",
    "        \"\"\"\n",
    "        Applies the normalization configuration in-place to a batch of inputs.\n",
    "        `x` is changed in-place since the function is mainly used internally\n",
    "        to standardize images and feed them to your network.\n",
    "        Args:\n",
    "            x: Batch of inputs to be normalized.\n",
    "        Returns:\n",
    "            The inputs, normalized. \n",
    "        \"\"\"\n",
    "        out = (x - self.dataset_mean)/self.dataset_std\n",
    "        out[out > 0] = out[out > 0]/self.norm_factor_pos\n",
    "        out[out < 0] = out[out < 0]/self.norm_factor_neg\n",
    "        out = np.clip(out, self.dataset_min, self.dataset_max)\n",
    "        return out\n",
    "\n",
    "    def save_batches_sequentially(self):\n",
    "        num_batches = self.__len__()\n",
    "        errors_found = []\n",
    "        for i in tqdm(range(num_batches), desc=\"Saving batches as TFRecords\"):\n",
    "            result = self.save_single_batch(i)\n",
    "            if \"Error\" in result:\n",
    "                print(result)\n",
    "                errors_found.append(result)\n",
    "        \n",
    "        if errors_found:\n",
    "            logging.warning(f\"Encountered {len(errors_found)} errors during sequential saving of TFRecords.\")\n",
    "        else:\n",
    "            logging.info(\"All batches saved successfully in sequential mode.\")\n",
    "\n",
    "\n",
    "    def save_single_batch(self, batch_index):\n",
    "        \"\"\"\n",
    "        Serializes and saves a single batch to a TFRecord file.\n",
    "        Args:\n",
    "            batch_index (int): Index of the batch to save.\n",
    "        Returns:\n",
    "            str: Path to the saved TFRecord file or an error message.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            filename = f\"batch_{batch_index}.tfrecord\"\n",
    "            TFRfile_path = os.path.join(self.tfrecords_dir, filename)\n",
    "            X, y = self.prepare_batch_data(batch_index)\n",
    "            serialized_example = self.serialize_example(X, y)\n",
    "            with tf.io.TFRecordWriter(TFRfile_path) as writer:\n",
    "                writer.write(serialized_example)\n",
    "            return TFRfile_path\n",
    "        except Exception as e:\n",
    "            return f\"Error saving batch {batch_index}: {e}\" \n",
    "      \n",
    "    @staticmethod  \n",
    "    def get_best_batch_size(file_offsets, target_bs=5000):\n",
    "        \"\"\"\n",
    "        Find the best batch size that minimizes the residual when dividing the total number of rows.\n",
    "        Args:\n",
    "            file_offsets (np.ndarray): Array of file offsets.\n",
    "            target_bs (int): Target batch size.\n",
    "            tol (float): Tolerance for batch size deviation.\n",
    "        Returns:\n",
    "            int: Best batch size.\n",
    "        \"\"\"\n",
    "        last_offset = file_offsets[-1]\n",
    "        d_bs = int(0.5 * target_bs)\n",
    "        batch_sizes = np.arange(target_bs - d_bs, target_bs + d_bs + 1)\n",
    "\n",
    "        residuals = last_offset % batch_sizes\n",
    "        min_res   = residuals.min()\n",
    "\n",
    "        # All bs giving the minimal residual\n",
    "        candidates = batch_sizes[residuals == min_res]\n",
    "\n",
    "        # Prefer the one closest to the target\n",
    "        idx = np.argmin(np.abs(candidates - target_bs))\n",
    "        return int(candidates[idx]), min_res    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_batching_plan(file_offsets, batch_size, tol = 0.75):\n",
    "        \"\"\"\n",
    "        Pre-compute (row_start, row_end) for every batch.\n",
    "        If the last batch < 0.5xbatch_size, merge the last two\n",
    "        and split them evenly, so both new batches are within\n",
    "        0.5x...1.0xbatch_size.\n",
    "        \"\"\"\n",
    "        total = file_offsets[-1]\n",
    "        b      = batch_size\n",
    "        plan   = []\n",
    "        start  = 0\n",
    "        while start < total:\n",
    "            end = min(start + b, total)\n",
    "            plan.append((start, end))\n",
    "            start = end\n",
    "\n",
    "        # Re-balance if the tail is too short\n",
    "        if len(plan) >= 2:\n",
    "            last_len = plan[-1][1] - plan[-1][0]\n",
    "            if last_len < tol * b:\n",
    "                sec_start = plan[-2][0]\n",
    "                comb_len  = plan[-1][1] - sec_start\n",
    "                half      = math.ceil(comb_len / 2)\n",
    "                plan[-2]  = (sec_start, sec_start + half)\n",
    "                plan[-1]  = (sec_start + half, sec_start + comb_len)\n",
    "        return plan\n",
    "    \n",
    "    @classmethod\n",
    "    def build_batch_metadata(cls, batch_size: int, file_offsets: np.ndarray, tail_tol: float = 0.75) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Builds optimized batch metadata using a pre-computed batch plan.\n",
    "        This ensures that the final batch is not excessively small.\n",
    "        \"\"\"\n",
    "        batching_plan = cls._build_batching_plan(file_offsets, batch_size, tail_tol)\n",
    "        batch_metadata = []\n",
    "\n",
    "        # 2. Loop through the generated plan instead of a simple range\n",
    "        for batch_index, (start_evt, end_evt) in enumerate(batching_plan):\n",
    "            \n",
    "            # Create a new dictionary for the current batch\n",
    "            current_batch_meta = {\n",
    "                \"batch_idx\": batch_index,\n",
    "                \"target_batch_size\": int(batch_size),\n",
    "                # The actual size is now simply the difference from the plan\n",
    "                \"actual_batch_size\": int(end_evt - start_evt),\n",
    "                \"segments\": []\n",
    "            }\n",
    "\n",
    "            # 3. Use the same logic as before to find the file segments for the given range\n",
    "            file_idx = np.searchsorted(file_offsets, start_evt, side=\"right\") - 1\n",
    "            evt_cursor = start_evt\n",
    "\n",
    "            while evt_cursor < end_evt:\n",
    "                file_start = file_offsets[file_idx]\n",
    "                file_end = file_offsets[file_idx + 1]\n",
    "\n",
    "                rel_start = evt_cursor - file_start\n",
    "                rel_end = min(end_evt, file_end) - file_start\n",
    "                \n",
    "                # Append segment info to the current batch's metadata\n",
    "                current_batch_meta[\"segments\"].append({\n",
    "                    \"file_idx\": int(file_idx),\n",
    "                    \"row_start\": int(rel_start),\n",
    "                    \"row_end\": int(rel_end - 1)\n",
    "                })\n",
    "                \n",
    "                evt_cursor += (rel_end - rel_start)\n",
    "                file_idx += 1\n",
    "            \n",
    "            batch_metadata.append(current_batch_meta)\n",
    "\n",
    "        return batch_metadata\n",
    " \n",
    "    def prepare_batch_data(self, batch_index):\n",
    "        batch_plan = self.batch_metadata[batch_index]\n",
    "\n",
    "        X_chunks = []\n",
    "        y_chunks = []\n",
    "\n",
    "        for segment in batch_plan[\"segments\"]:\n",
    "            file_idx = segment[\"file_idx\"]\n",
    "            rel_start = segment[\"row_start\"]\n",
    "            rel_end = segment[\"row_end\"] + 1  # inclusive end\n",
    "\n",
    "            if file_idx != self.current_file_index:\n",
    "                parquet_file = self.files[file_idx]\n",
    "                df = (pd.read_parquet(parquet_file,\n",
    "                                    columns=self.recon_cols + self.labels_list)\n",
    "                        .dropna(subset=self.recon_cols)\n",
    "                        .reset_index(drop=True))\n",
    "                if self.shuffle:\n",
    "                    df = df.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "                recon_df  = df[self.recon_cols]\n",
    "                labels_df = df[self.labels_list]\n",
    "\n",
    "                recon_values = recon_df.values\n",
    "                nonzeros = abs(recon_values) > 0\n",
    "                recon_values[nonzeros] = np.sign(recon_values[nonzeros]) * np.log1p(abs(recon_values[nonzeros])) / np.log(2)\n",
    "                if self.to_standardize:\n",
    "                    recon_values[nonzeros] = self.standardize(recon_values[nonzeros])\n",
    "                recon_values = recon_values.reshape((-1, *self.input_shape))\n",
    "                if self.transpose is not None:\n",
    "                    recon_values = recon_values.transpose(self.transpose)\n",
    "                self.current_dataframes = (\n",
    "                    recon_values, \n",
    "                    labels_df.values,\n",
    "                )\n",
    "                self.current_file_index = file_idx\n",
    "                del df\n",
    "                gc.collect()\n",
    "\n",
    "            recon_df, labels_df = self.current_dataframes\n",
    "            X_chunk = recon_df[rel_start:rel_end]\n",
    "            y_chunk = labels_df[rel_start:rel_end] / self.labels_scale\n",
    "\n",
    "            X_chunks.append(X_chunk)\n",
    "            y_chunks.append(y_chunk)\n",
    "\n",
    "\n",
    "\n",
    "        X = np.concatenate(X_chunks, axis=0)\n",
    "        y = np.concatenate(y_chunks, axis=0)\n",
    "\n",
    "        return X, y\n",
    "   \n",
    "\n",
    "    def serialize_example(self, X, y):\n",
    "        \"\"\"\n",
    "        Serializes a single example (featuresand labels) to TFRecord format. \n",
    "        \n",
    "        Args:\n",
    "        - X: Training data\n",
    "        - y: labelled data\n",
    "        \n",
    "        Returns:\n",
    "        - string (serialized TFRecord example).\n",
    "        \"\"\"\n",
    "        # X and y are float32 (maybe we can reduce this)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "\n",
    "        feature = {\n",
    "            'X': self._bytes_feature(tf.io.serialize_tensor(X)),\n",
    "            'y': self._bytes_feature(tf.io.serialize_tensor(y)),\n",
    "        }\n",
    "        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        return example_proto.SerializeToString()\n",
    "\n",
    "    @staticmethod\n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"\n",
    "        Converts a string/byte value into a Tf feature of bytes_list\n",
    "        \n",
    "        Args: \n",
    "        - string/byte value\n",
    "        \n",
    "        Returns:\n",
    "        - tf.train.Feature object as a bytes_list containing the input value.\n",
    "        \"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))): # check if Tf tensor\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        \"\"\"\n",
    "        Load the batch from a pre-saved TFRecord file instead of processing raw data.\n",
    "        Each file contains exactly one batch.\n",
    "        quantization is done here: Helpful for pretraining without the quantization and the later training with quantized data.\n",
    "        shuffling is also done here.\n",
    "        TODO: prefetching (un-done)\n",
    "        \"\"\"\n",
    "        tfrecord_path = self.tfrecord_filenames[batch_index]\n",
    "        raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "        parsed_dataset = raw_dataset.map(self._parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        # Get the first (and only) batch from the dataset\n",
    "        try:\n",
    "            X_batch, y_batch = next(iter(parsed_dataset))\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"No data found in TFRecord file: {tfrecord_path}\")\n",
    "\n",
    "        X_batch = tf.reshape(X_batch, [-1, *X_batch.shape[1:]])\n",
    "        y_batch = tf.reshape(y_batch, [-1, *y_batch.shape[1:]])\n",
    "\n",
    "        if self.quantize:\n",
    "            X_batch = QKeras_data_prep_quantizer(X_batch, bits=4, int_bits=0, alpha=1)\n",
    "\n",
    "        if self.shuffle:\n",
    "            indices = tf.range(start=0, limit=tf.shape(X_batch)[0], dtype=tf.int32)\n",
    "            shuffled_indices = tf.random.shuffle(indices, seed=self.seed)\n",
    "            X_batch = tf.gather(X_batch, shuffled_indices)\n",
    "            y_batch = tf.gather(y_batch, shuffled_indices)\n",
    "\n",
    "        del raw_dataset, parsed_dataset\n",
    "        return X_batch, y_batch\n",
    "            \n",
    "    @staticmethod\n",
    "    def _parse_tfrecord_fn(example):\n",
    "        \"\"\"\n",
    "        Parses a single TFRecord example.\n",
    "        \n",
    "        Returns:\n",
    "        - X: as a float32 tensor.\n",
    "        - y: as a float32 tensor.\n",
    "        \"\"\"\n",
    "        feature_description = {\n",
    "            'X': tf.io.FixedLenFeature([], tf.string),\n",
    "            'y': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        X = tf.io.parse_tensor(example['X'], out_type=tf.float32)\n",
    "        y = tf.io.parse_tensor(example['y'], out_type=tf.float32)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Phase-aware length:\n",
    "            during initial TFRecord creation: math on file_offsets\n",
    "            after creation in same process: len(batch_metadata)\n",
    "            when loading existing TFRecords: len(tfrecord_filenames)\n",
    "        \"\"\"\n",
    "        # already have metadata?  Fastest answer.\n",
    "        if self.batch_metadata:\n",
    "            return len(self.batch_metadata)\n",
    "\n",
    "        # still building batches, so compute from source rows.\n",
    "        if len(self.file_offsets) > 1:         # have real offsets\n",
    "            total_rows = self.file_offsets[-1]\n",
    "            return math.ceil(total_rows / self.batch_size)\n",
    "\n",
    "        # running in \"load\" mode.\n",
    "        self.tfrecord_filenames = np.sort(\n",
    "            np.array(tf.io.gfile.glob(\n",
    "                os.path.join(self.tfrecords_dir, \"*.tfrecord\"))))\n",
    "        return len(self.tfrecord_filenames)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        This shuffles the file ordering so that it shuffles the ordering in which the TFRecord\n",
    "        are loaded during the training for each epochs.\n",
    "        '''\n",
    "        gc.collect()\n",
    "        self.epoch_count += 1\n",
    "        # Log quantization status once\n",
    "        if self.epoch_count == 1:\n",
    "            logging.warning(f\"Quantization is {self.quantize} in data generator. This may affect model performance.\")\n",
    "\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(self.tfrecord_filenames)\n",
    "            self.seed += 1 # So that after each epoch the batch is shuffled with a different seed (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd94db45-10a3-4aa0-9885-cad5c6cc25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"TFR_files\", \"2t\")\n",
    "\n",
    "dataset_base_dir = os.path.join(dataset_base_dir, \"parquets\")\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\")\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\")\n",
    "\n",
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 75\n",
    "val_file_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccba057-7aa7-4b85-95bd-8977c14cf7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 25/25 [00:14<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_val is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords:   0%|          | 0/102 [00:00<?, ?it/s]2025-06-07 01:20:28.901518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3234 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB MIG 1g.5gb, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "Saving batches as TFRecords: 100%|██████████| 102/102 [00:25<00:00,  4.03it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation generator 40.57191061973572 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 75/75 [00:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_train is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords: 100%|██████████| 306/306 [01:12<00:00,  4.24it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training generator 117.55884718894958 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fb2dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,   20374,   40748,   61122,   81496,  101870,  122244,\n",
       "        142618,  162992,  183366,  203740,  224114,  244488,  264862,\n",
       "        285236,  305610,  325984,  346358,  366732,  387106,  407480,\n",
       "        427854,  448228,  468602,  488976,  509350,  529724,  550098,\n",
       "        570472,  590846,  611220,  631594,  651968,  672342,  692716,\n",
       "        713090,  733464,  753838,  774212,  794586,  814960,  835334,\n",
       "        855708,  876082,  896456,  916830,  937204,  957578,  977952,\n",
       "        998326, 1018700, 1039074, 1059448, 1079822, 1100196, 1120570,\n",
       "       1140944, 1161318, 1181692, 1202066, 1222440, 1242814, 1263188,\n",
       "       1283562, 1303936, 1324310, 1344684, 1365058, 1385432, 1405806,\n",
       "       1426180, 1446554, 1466928, 1487302, 1507676, 1528050])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.file_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880dc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking auto normalization factors:\n",
      "Training generator norm factor pos: 1.4030861830865065\n",
      "Training generator norm factor neg: 2.4879350274661034\n",
      "\n",
      "Checking auto scale factors:\n",
      "Training generator labels scale: [74.36465612 18.59104573  8.60158289  0.53649678]\n",
      "Training generator dataset mean: [5.02385156]\n",
      "Training generator dataset std: [6.11337844]\n",
      "Training generator dataset max: 1.9337638243125104\n",
      "Training generator dataset min: -2.8022317516663517\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking auto normalization factors:\")\n",
    "print(\"Training generator norm factor pos:\", training_generator.norm_factor_pos)\n",
    "print(\"Training generator norm factor neg:\", training_generator.norm_factor_neg)\n",
    "print()\n",
    "print(\"Checking auto scale factors:\")\n",
    "print(\"Training generator labels scale:\", training_generator.labels_scale)\n",
    "print(\"Training generator dataset mean:\", training_generator.dataset_mean)\n",
    "print(\"Training generator dataset std:\", training_generator.dataset_std)\n",
    "print(\"Training generator dataset max:\", training_generator.dataset_max)\n",
    "print(\"Training generator dataset min:\", training_generator.dataset_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "077183f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_idx': 0,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 0, 'row_end': 4999}]},\n",
       " {'batch_idx': 1,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 5000, 'row_end': 9999}]},\n",
       " {'batch_idx': 2,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 10000, 'row_end': 14999}]},\n",
       " {'batch_idx': 3,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 15000, 'row_end': 19999}]},\n",
       " {'batch_idx': 4,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 0, 'row_start': 20000, 'row_end': 20373},\n",
       "   {'file_idx': 1, 'row_start': 0, 'row_end': 4625}]},\n",
       " {'batch_idx': 5,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 4626, 'row_end': 9625}]},\n",
       " {'batch_idx': 6,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 9626, 'row_end': 14625}]},\n",
       " {'batch_idx': 7,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 14626, 'row_end': 19625}]},\n",
       " {'batch_idx': 8,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 1, 'row_start': 19626, 'row_end': 20373},\n",
       "   {'file_idx': 2, 'row_start': 0, 'row_end': 4251}]},\n",
       " {'batch_idx': 9,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 4252, 'row_end': 9251}]},\n",
       " {'batch_idx': 10,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 9252, 'row_end': 14251}]},\n",
       " {'batch_idx': 11,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 14252, 'row_end': 19251}]},\n",
       " {'batch_idx': 12,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 2, 'row_start': 19252, 'row_end': 20373},\n",
       "   {'file_idx': 3, 'row_start': 0, 'row_end': 3877}]},\n",
       " {'batch_idx': 13,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 3878, 'row_end': 8877}]},\n",
       " {'batch_idx': 14,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 8878, 'row_end': 13877}]},\n",
       " {'batch_idx': 15,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 13878, 'row_end': 18877}]},\n",
       " {'batch_idx': 16,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 3, 'row_start': 18878, 'row_end': 20373},\n",
       "   {'file_idx': 4, 'row_start': 0, 'row_end': 3503}]},\n",
       " {'batch_idx': 17,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 3504, 'row_end': 8503}]},\n",
       " {'batch_idx': 18,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 8504, 'row_end': 13503}]},\n",
       " {'batch_idx': 19,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 13504, 'row_end': 18503}]},\n",
       " {'batch_idx': 20,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 4, 'row_start': 18504, 'row_end': 20373},\n",
       "   {'file_idx': 5, 'row_start': 0, 'row_end': 3129}]},\n",
       " {'batch_idx': 21,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 3130, 'row_end': 8129}]},\n",
       " {'batch_idx': 22,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 8130, 'row_end': 13129}]},\n",
       " {'batch_idx': 23,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 13130, 'row_end': 18129}]},\n",
       " {'batch_idx': 24,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 5, 'row_start': 18130, 'row_end': 20373},\n",
       "   {'file_idx': 6, 'row_start': 0, 'row_end': 2755}]},\n",
       " {'batch_idx': 25,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 2756, 'row_end': 7755}]},\n",
       " {'batch_idx': 26,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 7756, 'row_end': 12755}]},\n",
       " {'batch_idx': 27,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 12756, 'row_end': 17755}]},\n",
       " {'batch_idx': 28,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 6, 'row_start': 17756, 'row_end': 20373},\n",
       "   {'file_idx': 7, 'row_start': 0, 'row_end': 2381}]},\n",
       " {'batch_idx': 29,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 2382, 'row_end': 7381}]},\n",
       " {'batch_idx': 30,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 7382, 'row_end': 12381}]},\n",
       " {'batch_idx': 31,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 12382, 'row_end': 17381}]},\n",
       " {'batch_idx': 32,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 7, 'row_start': 17382, 'row_end': 20373},\n",
       "   {'file_idx': 8, 'row_start': 0, 'row_end': 2007}]},\n",
       " {'batch_idx': 33,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 2008, 'row_end': 7007}]},\n",
       " {'batch_idx': 34,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 7008, 'row_end': 12007}]},\n",
       " {'batch_idx': 35,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 12008, 'row_end': 17007}]},\n",
       " {'batch_idx': 36,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 8, 'row_start': 17008, 'row_end': 20373},\n",
       "   {'file_idx': 9, 'row_start': 0, 'row_end': 1633}]},\n",
       " {'batch_idx': 37,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 1634, 'row_end': 6633}]},\n",
       " {'batch_idx': 38,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 6634, 'row_end': 11633}]},\n",
       " {'batch_idx': 39,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 11634, 'row_end': 16633}]},\n",
       " {'batch_idx': 40,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 9, 'row_start': 16634, 'row_end': 20373},\n",
       "   {'file_idx': 10, 'row_start': 0, 'row_end': 1259}]},\n",
       " {'batch_idx': 41,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 1260, 'row_end': 6259}]},\n",
       " {'batch_idx': 42,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 6260, 'row_end': 11259}]},\n",
       " {'batch_idx': 43,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 11260, 'row_end': 16259}]},\n",
       " {'batch_idx': 44,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 10, 'row_start': 16260, 'row_end': 20373},\n",
       "   {'file_idx': 11, 'row_start': 0, 'row_end': 885}]},\n",
       " {'batch_idx': 45,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 886, 'row_end': 5885}]},\n",
       " {'batch_idx': 46,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 5886, 'row_end': 10885}]},\n",
       " {'batch_idx': 47,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 10886, 'row_end': 15885}]},\n",
       " {'batch_idx': 48,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 11, 'row_start': 15886, 'row_end': 20373},\n",
       "   {'file_idx': 12, 'row_start': 0, 'row_end': 511}]},\n",
       " {'batch_idx': 49,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 512, 'row_end': 5511}]},\n",
       " {'batch_idx': 50,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 5512, 'row_end': 10511}]},\n",
       " {'batch_idx': 51,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 10512, 'row_end': 15511}]},\n",
       " {'batch_idx': 52,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 12, 'row_start': 15512, 'row_end': 20373},\n",
       "   {'file_idx': 13, 'row_start': 0, 'row_end': 137}]},\n",
       " {'batch_idx': 53,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 138, 'row_end': 5137}]},\n",
       " {'batch_idx': 54,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 5138, 'row_end': 10137}]},\n",
       " {'batch_idx': 55,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 10138, 'row_end': 15137}]},\n",
       " {'batch_idx': 56,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 15138, 'row_end': 20137}]},\n",
       " {'batch_idx': 57,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 13, 'row_start': 20138, 'row_end': 20373},\n",
       "   {'file_idx': 14, 'row_start': 0, 'row_end': 4763}]},\n",
       " {'batch_idx': 58,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 4764, 'row_end': 9763}]},\n",
       " {'batch_idx': 59,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 9764, 'row_end': 14763}]},\n",
       " {'batch_idx': 60,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 14764, 'row_end': 19763}]},\n",
       " {'batch_idx': 61,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 14, 'row_start': 19764, 'row_end': 20373},\n",
       "   {'file_idx': 15, 'row_start': 0, 'row_end': 4389}]},\n",
       " {'batch_idx': 62,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 4390, 'row_end': 9389}]},\n",
       " {'batch_idx': 63,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 9390, 'row_end': 14389}]},\n",
       " {'batch_idx': 64,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 14390, 'row_end': 19389}]},\n",
       " {'batch_idx': 65,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 15, 'row_start': 19390, 'row_end': 20373},\n",
       "   {'file_idx': 16, 'row_start': 0, 'row_end': 4015}]},\n",
       " {'batch_idx': 66,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 4016, 'row_end': 9015}]},\n",
       " {'batch_idx': 67,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 9016, 'row_end': 14015}]},\n",
       " {'batch_idx': 68,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 14016, 'row_end': 19015}]},\n",
       " {'batch_idx': 69,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 16, 'row_start': 19016, 'row_end': 20373},\n",
       "   {'file_idx': 17, 'row_start': 0, 'row_end': 3641}]},\n",
       " {'batch_idx': 70,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 3642, 'row_end': 8641}]},\n",
       " {'batch_idx': 71,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 8642, 'row_end': 13641}]},\n",
       " {'batch_idx': 72,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 13642, 'row_end': 18641}]},\n",
       " {'batch_idx': 73,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 17, 'row_start': 18642, 'row_end': 20373},\n",
       "   {'file_idx': 18, 'row_start': 0, 'row_end': 3267}]},\n",
       " {'batch_idx': 74,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 3268, 'row_end': 8267}]},\n",
       " {'batch_idx': 75,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 8268, 'row_end': 13267}]},\n",
       " {'batch_idx': 76,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 13268, 'row_end': 18267}]},\n",
       " {'batch_idx': 77,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 18, 'row_start': 18268, 'row_end': 20373},\n",
       "   {'file_idx': 19, 'row_start': 0, 'row_end': 2893}]},\n",
       " {'batch_idx': 78,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 2894, 'row_end': 7893}]},\n",
       " {'batch_idx': 79,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 7894, 'row_end': 12893}]},\n",
       " {'batch_idx': 80,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 12894, 'row_end': 17893}]},\n",
       " {'batch_idx': 81,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 19, 'row_start': 17894, 'row_end': 20373},\n",
       "   {'file_idx': 20, 'row_start': 0, 'row_end': 2519}]},\n",
       " {'batch_idx': 82,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 2520, 'row_end': 7519}]},\n",
       " {'batch_idx': 83,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 7520, 'row_end': 12519}]},\n",
       " {'batch_idx': 84,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 12520, 'row_end': 17519}]},\n",
       " {'batch_idx': 85,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 20, 'row_start': 17520, 'row_end': 20373},\n",
       "   {'file_idx': 21, 'row_start': 0, 'row_end': 2145}]},\n",
       " {'batch_idx': 86,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 2146, 'row_end': 7145}]},\n",
       " {'batch_idx': 87,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 7146, 'row_end': 12145}]},\n",
       " {'batch_idx': 88,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 12146, 'row_end': 17145}]},\n",
       " {'batch_idx': 89,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 21, 'row_start': 17146, 'row_end': 20373},\n",
       "   {'file_idx': 22, 'row_start': 0, 'row_end': 1771}]},\n",
       " {'batch_idx': 90,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 1772, 'row_end': 6771}]},\n",
       " {'batch_idx': 91,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 6772, 'row_end': 11771}]},\n",
       " {'batch_idx': 92,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 11772, 'row_end': 16771}]},\n",
       " {'batch_idx': 93,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 22, 'row_start': 16772, 'row_end': 20373},\n",
       "   {'file_idx': 23, 'row_start': 0, 'row_end': 1397}]},\n",
       " {'batch_idx': 94,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 1398, 'row_end': 6397}]},\n",
       " {'batch_idx': 95,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 6398, 'row_end': 11397}]},\n",
       " {'batch_idx': 96,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 11398, 'row_end': 16397}]},\n",
       " {'batch_idx': 97,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 23, 'row_start': 16398, 'row_end': 20373},\n",
       "   {'file_idx': 24, 'row_start': 0, 'row_end': 1023}]},\n",
       " {'batch_idx': 98,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 1024, 'row_end': 6023}]},\n",
       " {'batch_idx': 99,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 6024, 'row_end': 11023}]},\n",
       " {'batch_idx': 100,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 11024, 'row_end': 16023}]},\n",
       " {'batch_idx': 101,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 24, 'row_start': 16024, 'row_end': 20373},\n",
       "   {'file_idx': 25, 'row_start': 0, 'row_end': 649}]},\n",
       " {'batch_idx': 102,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 650, 'row_end': 5649}]},\n",
       " {'batch_idx': 103,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 5650, 'row_end': 10649}]},\n",
       " {'batch_idx': 104,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 10650, 'row_end': 15649}]},\n",
       " {'batch_idx': 105,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 25, 'row_start': 15650, 'row_end': 20373},\n",
       "   {'file_idx': 26, 'row_start': 0, 'row_end': 275}]},\n",
       " {'batch_idx': 106,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 276, 'row_end': 5275}]},\n",
       " {'batch_idx': 107,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 5276, 'row_end': 10275}]},\n",
       " {'batch_idx': 108,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 10276, 'row_end': 15275}]},\n",
       " {'batch_idx': 109,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 15276, 'row_end': 20275}]},\n",
       " {'batch_idx': 110,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 26, 'row_start': 20276, 'row_end': 20373},\n",
       "   {'file_idx': 27, 'row_start': 0, 'row_end': 4901}]},\n",
       " {'batch_idx': 111,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 4902, 'row_end': 9901}]},\n",
       " {'batch_idx': 112,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 9902, 'row_end': 14901}]},\n",
       " {'batch_idx': 113,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 14902, 'row_end': 19901}]},\n",
       " {'batch_idx': 114,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 27, 'row_start': 19902, 'row_end': 20373},\n",
       "   {'file_idx': 28, 'row_start': 0, 'row_end': 4527}]},\n",
       " {'batch_idx': 115,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 4528, 'row_end': 9527}]},\n",
       " {'batch_idx': 116,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 9528, 'row_end': 14527}]},\n",
       " {'batch_idx': 117,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 14528, 'row_end': 19527}]},\n",
       " {'batch_idx': 118,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 28, 'row_start': 19528, 'row_end': 20373},\n",
       "   {'file_idx': 29, 'row_start': 0, 'row_end': 4153}]},\n",
       " {'batch_idx': 119,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 4154, 'row_end': 9153}]},\n",
       " {'batch_idx': 120,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 9154, 'row_end': 14153}]},\n",
       " {'batch_idx': 121,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 14154, 'row_end': 19153}]},\n",
       " {'batch_idx': 122,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 29, 'row_start': 19154, 'row_end': 20373},\n",
       "   {'file_idx': 30, 'row_start': 0, 'row_end': 3779}]},\n",
       " {'batch_idx': 123,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 3780, 'row_end': 8779}]},\n",
       " {'batch_idx': 124,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 8780, 'row_end': 13779}]},\n",
       " {'batch_idx': 125,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 13780, 'row_end': 18779}]},\n",
       " {'batch_idx': 126,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 30, 'row_start': 18780, 'row_end': 20373},\n",
       "   {'file_idx': 31, 'row_start': 0, 'row_end': 3405}]},\n",
       " {'batch_idx': 127,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 3406, 'row_end': 8405}]},\n",
       " {'batch_idx': 128,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 8406, 'row_end': 13405}]},\n",
       " {'batch_idx': 129,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 13406, 'row_end': 18405}]},\n",
       " {'batch_idx': 130,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 31, 'row_start': 18406, 'row_end': 20373},\n",
       "   {'file_idx': 32, 'row_start': 0, 'row_end': 3031}]},\n",
       " {'batch_idx': 131,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 3032, 'row_end': 8031}]},\n",
       " {'batch_idx': 132,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 8032, 'row_end': 13031}]},\n",
       " {'batch_idx': 133,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 13032, 'row_end': 18031}]},\n",
       " {'batch_idx': 134,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 32, 'row_start': 18032, 'row_end': 20373},\n",
       "   {'file_idx': 33, 'row_start': 0, 'row_end': 2657}]},\n",
       " {'batch_idx': 135,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 2658, 'row_end': 7657}]},\n",
       " {'batch_idx': 136,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 7658, 'row_end': 12657}]},\n",
       " {'batch_idx': 137,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 12658, 'row_end': 17657}]},\n",
       " {'batch_idx': 138,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 33, 'row_start': 17658, 'row_end': 20373},\n",
       "   {'file_idx': 34, 'row_start': 0, 'row_end': 2283}]},\n",
       " {'batch_idx': 139,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 2284, 'row_end': 7283}]},\n",
       " {'batch_idx': 140,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 7284, 'row_end': 12283}]},\n",
       " {'batch_idx': 141,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 12284, 'row_end': 17283}]},\n",
       " {'batch_idx': 142,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 34, 'row_start': 17284, 'row_end': 20373},\n",
       "   {'file_idx': 35, 'row_start': 0, 'row_end': 1909}]},\n",
       " {'batch_idx': 143,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 1910, 'row_end': 6909}]},\n",
       " {'batch_idx': 144,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 6910, 'row_end': 11909}]},\n",
       " {'batch_idx': 145,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 11910, 'row_end': 16909}]},\n",
       " {'batch_idx': 146,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 35, 'row_start': 16910, 'row_end': 20373},\n",
       "   {'file_idx': 36, 'row_start': 0, 'row_end': 1535}]},\n",
       " {'batch_idx': 147,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 1536, 'row_end': 6535}]},\n",
       " {'batch_idx': 148,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 6536, 'row_end': 11535}]},\n",
       " {'batch_idx': 149,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 11536, 'row_end': 16535}]},\n",
       " {'batch_idx': 150,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 36, 'row_start': 16536, 'row_end': 20373},\n",
       "   {'file_idx': 37, 'row_start': 0, 'row_end': 1161}]},\n",
       " {'batch_idx': 151,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 1162, 'row_end': 6161}]},\n",
       " {'batch_idx': 152,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 6162, 'row_end': 11161}]},\n",
       " {'batch_idx': 153,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 11162, 'row_end': 16161}]},\n",
       " {'batch_idx': 154,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 37, 'row_start': 16162, 'row_end': 20373},\n",
       "   {'file_idx': 38, 'row_start': 0, 'row_end': 787}]},\n",
       " {'batch_idx': 155,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 788, 'row_end': 5787}]},\n",
       " {'batch_idx': 156,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 5788, 'row_end': 10787}]},\n",
       " {'batch_idx': 157,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 10788, 'row_end': 15787}]},\n",
       " {'batch_idx': 158,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 38, 'row_start': 15788, 'row_end': 20373},\n",
       "   {'file_idx': 39, 'row_start': 0, 'row_end': 413}]},\n",
       " {'batch_idx': 159,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 414, 'row_end': 5413}]},\n",
       " {'batch_idx': 160,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 5414, 'row_end': 10413}]},\n",
       " {'batch_idx': 161,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 10414, 'row_end': 15413}]},\n",
       " {'batch_idx': 162,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 39, 'row_start': 15414, 'row_end': 20373},\n",
       "   {'file_idx': 40, 'row_start': 0, 'row_end': 39}]},\n",
       " {'batch_idx': 163,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 40, 'row_end': 5039}]},\n",
       " {'batch_idx': 164,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 5040, 'row_end': 10039}]},\n",
       " {'batch_idx': 165,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 10040, 'row_end': 15039}]},\n",
       " {'batch_idx': 166,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 15040, 'row_end': 20039}]},\n",
       " {'batch_idx': 167,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 40, 'row_start': 20040, 'row_end': 20373},\n",
       "   {'file_idx': 41, 'row_start': 0, 'row_end': 4665}]},\n",
       " {'batch_idx': 168,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 4666, 'row_end': 9665}]},\n",
       " {'batch_idx': 169,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 9666, 'row_end': 14665}]},\n",
       " {'batch_idx': 170,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 14666, 'row_end': 19665}]},\n",
       " {'batch_idx': 171,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 41, 'row_start': 19666, 'row_end': 20373},\n",
       "   {'file_idx': 42, 'row_start': 0, 'row_end': 4291}]},\n",
       " {'batch_idx': 172,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 4292, 'row_end': 9291}]},\n",
       " {'batch_idx': 173,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 9292, 'row_end': 14291}]},\n",
       " {'batch_idx': 174,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 14292, 'row_end': 19291}]},\n",
       " {'batch_idx': 175,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 42, 'row_start': 19292, 'row_end': 20373},\n",
       "   {'file_idx': 43, 'row_start': 0, 'row_end': 3917}]},\n",
       " {'batch_idx': 176,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 3918, 'row_end': 8917}]},\n",
       " {'batch_idx': 177,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 8918, 'row_end': 13917}]},\n",
       " {'batch_idx': 178,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 13918, 'row_end': 18917}]},\n",
       " {'batch_idx': 179,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 43, 'row_start': 18918, 'row_end': 20373},\n",
       "   {'file_idx': 44, 'row_start': 0, 'row_end': 3543}]},\n",
       " {'batch_idx': 180,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 3544, 'row_end': 8543}]},\n",
       " {'batch_idx': 181,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 8544, 'row_end': 13543}]},\n",
       " {'batch_idx': 182,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 13544, 'row_end': 18543}]},\n",
       " {'batch_idx': 183,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 44, 'row_start': 18544, 'row_end': 20373},\n",
       "   {'file_idx': 45, 'row_start': 0, 'row_end': 3169}]},\n",
       " {'batch_idx': 184,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 3170, 'row_end': 8169}]},\n",
       " {'batch_idx': 185,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 8170, 'row_end': 13169}]},\n",
       " {'batch_idx': 186,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 13170, 'row_end': 18169}]},\n",
       " {'batch_idx': 187,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 45, 'row_start': 18170, 'row_end': 20373},\n",
       "   {'file_idx': 46, 'row_start': 0, 'row_end': 2795}]},\n",
       " {'batch_idx': 188,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 2796, 'row_end': 7795}]},\n",
       " {'batch_idx': 189,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 7796, 'row_end': 12795}]},\n",
       " {'batch_idx': 190,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 12796, 'row_end': 17795}]},\n",
       " {'batch_idx': 191,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 46, 'row_start': 17796, 'row_end': 20373},\n",
       "   {'file_idx': 47, 'row_start': 0, 'row_end': 2421}]},\n",
       " {'batch_idx': 192,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 2422, 'row_end': 7421}]},\n",
       " {'batch_idx': 193,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 7422, 'row_end': 12421}]},\n",
       " {'batch_idx': 194,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 12422, 'row_end': 17421}]},\n",
       " {'batch_idx': 195,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 47, 'row_start': 17422, 'row_end': 20373},\n",
       "   {'file_idx': 48, 'row_start': 0, 'row_end': 2047}]},\n",
       " {'batch_idx': 196,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 2048, 'row_end': 7047}]},\n",
       " {'batch_idx': 197,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 7048, 'row_end': 12047}]},\n",
       " {'batch_idx': 198,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 12048, 'row_end': 17047}]},\n",
       " {'batch_idx': 199,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 48, 'row_start': 17048, 'row_end': 20373},\n",
       "   {'file_idx': 49, 'row_start': 0, 'row_end': 1673}]},\n",
       " {'batch_idx': 200,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 1674, 'row_end': 6673}]},\n",
       " {'batch_idx': 201,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 6674, 'row_end': 11673}]},\n",
       " {'batch_idx': 202,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 11674, 'row_end': 16673}]},\n",
       " {'batch_idx': 203,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 49, 'row_start': 16674, 'row_end': 20373},\n",
       "   {'file_idx': 50, 'row_start': 0, 'row_end': 1299}]},\n",
       " {'batch_idx': 204,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 1300, 'row_end': 6299}]},\n",
       " {'batch_idx': 205,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 6300, 'row_end': 11299}]},\n",
       " {'batch_idx': 206,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 11300, 'row_end': 16299}]},\n",
       " {'batch_idx': 207,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 50, 'row_start': 16300, 'row_end': 20373},\n",
       "   {'file_idx': 51, 'row_start': 0, 'row_end': 925}]},\n",
       " {'batch_idx': 208,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 926, 'row_end': 5925}]},\n",
       " {'batch_idx': 209,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 5926, 'row_end': 10925}]},\n",
       " {'batch_idx': 210,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 10926, 'row_end': 15925}]},\n",
       " {'batch_idx': 211,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 51, 'row_start': 15926, 'row_end': 20373},\n",
       "   {'file_idx': 52, 'row_start': 0, 'row_end': 551}]},\n",
       " {'batch_idx': 212,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 552, 'row_end': 5551}]},\n",
       " {'batch_idx': 213,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 5552, 'row_end': 10551}]},\n",
       " {'batch_idx': 214,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 10552, 'row_end': 15551}]},\n",
       " {'batch_idx': 215,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 52, 'row_start': 15552, 'row_end': 20373},\n",
       "   {'file_idx': 53, 'row_start': 0, 'row_end': 177}]},\n",
       " {'batch_idx': 216,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 178, 'row_end': 5177}]},\n",
       " {'batch_idx': 217,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 5178, 'row_end': 10177}]},\n",
       " {'batch_idx': 218,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 10178, 'row_end': 15177}]},\n",
       " {'batch_idx': 219,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 15178, 'row_end': 20177}]},\n",
       " {'batch_idx': 220,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 53, 'row_start': 20178, 'row_end': 20373},\n",
       "   {'file_idx': 54, 'row_start': 0, 'row_end': 4803}]},\n",
       " {'batch_idx': 221,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 4804, 'row_end': 9803}]},\n",
       " {'batch_idx': 222,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 9804, 'row_end': 14803}]},\n",
       " {'batch_idx': 223,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 14804, 'row_end': 19803}]},\n",
       " {'batch_idx': 224,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 54, 'row_start': 19804, 'row_end': 20373},\n",
       "   {'file_idx': 55, 'row_start': 0, 'row_end': 4429}]},\n",
       " {'batch_idx': 225,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 4430, 'row_end': 9429}]},\n",
       " {'batch_idx': 226,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 9430, 'row_end': 14429}]},\n",
       " {'batch_idx': 227,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 14430, 'row_end': 19429}]},\n",
       " {'batch_idx': 228,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 55, 'row_start': 19430, 'row_end': 20373},\n",
       "   {'file_idx': 56, 'row_start': 0, 'row_end': 4055}]},\n",
       " {'batch_idx': 229,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 4056, 'row_end': 9055}]},\n",
       " {'batch_idx': 230,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 9056, 'row_end': 14055}]},\n",
       " {'batch_idx': 231,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 14056, 'row_end': 19055}]},\n",
       " {'batch_idx': 232,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 56, 'row_start': 19056, 'row_end': 20373},\n",
       "   {'file_idx': 57, 'row_start': 0, 'row_end': 3681}]},\n",
       " {'batch_idx': 233,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 3682, 'row_end': 8681}]},\n",
       " {'batch_idx': 234,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 8682, 'row_end': 13681}]},\n",
       " {'batch_idx': 235,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 13682, 'row_end': 18681}]},\n",
       " {'batch_idx': 236,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 57, 'row_start': 18682, 'row_end': 20373},\n",
       "   {'file_idx': 58, 'row_start': 0, 'row_end': 3307}]},\n",
       " {'batch_idx': 237,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 3308, 'row_end': 8307}]},\n",
       " {'batch_idx': 238,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 8308, 'row_end': 13307}]},\n",
       " {'batch_idx': 239,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 13308, 'row_end': 18307}]},\n",
       " {'batch_idx': 240,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 58, 'row_start': 18308, 'row_end': 20373},\n",
       "   {'file_idx': 59, 'row_start': 0, 'row_end': 2933}]},\n",
       " {'batch_idx': 241,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 2934, 'row_end': 7933}]},\n",
       " {'batch_idx': 242,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 7934, 'row_end': 12933}]},\n",
       " {'batch_idx': 243,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 12934, 'row_end': 17933}]},\n",
       " {'batch_idx': 244,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 59, 'row_start': 17934, 'row_end': 20373},\n",
       "   {'file_idx': 60, 'row_start': 0, 'row_end': 2559}]},\n",
       " {'batch_idx': 245,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 2560, 'row_end': 7559}]},\n",
       " {'batch_idx': 246,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 7560, 'row_end': 12559}]},\n",
       " {'batch_idx': 247,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 12560, 'row_end': 17559}]},\n",
       " {'batch_idx': 248,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 60, 'row_start': 17560, 'row_end': 20373},\n",
       "   {'file_idx': 61, 'row_start': 0, 'row_end': 2185}]},\n",
       " {'batch_idx': 249,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 2186, 'row_end': 7185}]},\n",
       " {'batch_idx': 250,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 7186, 'row_end': 12185}]},\n",
       " {'batch_idx': 251,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 12186, 'row_end': 17185}]},\n",
       " {'batch_idx': 252,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 61, 'row_start': 17186, 'row_end': 20373},\n",
       "   {'file_idx': 62, 'row_start': 0, 'row_end': 1811}]},\n",
       " {'batch_idx': 253,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 1812, 'row_end': 6811}]},\n",
       " {'batch_idx': 254,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 6812, 'row_end': 11811}]},\n",
       " {'batch_idx': 255,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 11812, 'row_end': 16811}]},\n",
       " {'batch_idx': 256,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 62, 'row_start': 16812, 'row_end': 20373},\n",
       "   {'file_idx': 63, 'row_start': 0, 'row_end': 1437}]},\n",
       " {'batch_idx': 257,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 1438, 'row_end': 6437}]},\n",
       " {'batch_idx': 258,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 6438, 'row_end': 11437}]},\n",
       " {'batch_idx': 259,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 11438, 'row_end': 16437}]},\n",
       " {'batch_idx': 260,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 63, 'row_start': 16438, 'row_end': 20373},\n",
       "   {'file_idx': 64, 'row_start': 0, 'row_end': 1063}]},\n",
       " {'batch_idx': 261,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 1064, 'row_end': 6063}]},\n",
       " {'batch_idx': 262,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 6064, 'row_end': 11063}]},\n",
       " {'batch_idx': 263,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 11064, 'row_end': 16063}]},\n",
       " {'batch_idx': 264,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 64, 'row_start': 16064, 'row_end': 20373},\n",
       "   {'file_idx': 65, 'row_start': 0, 'row_end': 689}]},\n",
       " {'batch_idx': 265,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 690, 'row_end': 5689}]},\n",
       " {'batch_idx': 266,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 5690, 'row_end': 10689}]},\n",
       " {'batch_idx': 267,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 10690, 'row_end': 15689}]},\n",
       " {'batch_idx': 268,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 65, 'row_start': 15690, 'row_end': 20373},\n",
       "   {'file_idx': 66, 'row_start': 0, 'row_end': 315}]},\n",
       " {'batch_idx': 269,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 316, 'row_end': 5315}]},\n",
       " {'batch_idx': 270,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 5316, 'row_end': 10315}]},\n",
       " {'batch_idx': 271,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 10316, 'row_end': 15315}]},\n",
       " {'batch_idx': 272,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 15316, 'row_end': 20315}]},\n",
       " {'batch_idx': 273,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 66, 'row_start': 20316, 'row_end': 20373},\n",
       "   {'file_idx': 67, 'row_start': 0, 'row_end': 4941}]},\n",
       " {'batch_idx': 274,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 4942, 'row_end': 9941}]},\n",
       " {'batch_idx': 275,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 9942, 'row_end': 14941}]},\n",
       " {'batch_idx': 276,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 14942, 'row_end': 19941}]},\n",
       " {'batch_idx': 277,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 67, 'row_start': 19942, 'row_end': 20373},\n",
       "   {'file_idx': 68, 'row_start': 0, 'row_end': 4567}]},\n",
       " {'batch_idx': 278,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 4568, 'row_end': 9567}]},\n",
       " {'batch_idx': 279,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 9568, 'row_end': 14567}]},\n",
       " {'batch_idx': 280,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 14568, 'row_end': 19567}]},\n",
       " {'batch_idx': 281,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 68, 'row_start': 19568, 'row_end': 20373},\n",
       "   {'file_idx': 69, 'row_start': 0, 'row_end': 4193}]},\n",
       " {'batch_idx': 282,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 4194, 'row_end': 9193}]},\n",
       " {'batch_idx': 283,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 9194, 'row_end': 14193}]},\n",
       " {'batch_idx': 284,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 14194, 'row_end': 19193}]},\n",
       " {'batch_idx': 285,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 69, 'row_start': 19194, 'row_end': 20373},\n",
       "   {'file_idx': 70, 'row_start': 0, 'row_end': 3819}]},\n",
       " {'batch_idx': 286,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 3820, 'row_end': 8819}]},\n",
       " {'batch_idx': 287,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 8820, 'row_end': 13819}]},\n",
       " {'batch_idx': 288,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 13820, 'row_end': 18819}]},\n",
       " {'batch_idx': 289,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 70, 'row_start': 18820, 'row_end': 20373},\n",
       "   {'file_idx': 71, 'row_start': 0, 'row_end': 3445}]},\n",
       " {'batch_idx': 290,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 3446, 'row_end': 8445}]},\n",
       " {'batch_idx': 291,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 8446, 'row_end': 13445}]},\n",
       " {'batch_idx': 292,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 13446, 'row_end': 18445}]},\n",
       " {'batch_idx': 293,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 71, 'row_start': 18446, 'row_end': 20373},\n",
       "   {'file_idx': 72, 'row_start': 0, 'row_end': 3071}]},\n",
       " {'batch_idx': 294,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 3072, 'row_end': 8071}]},\n",
       " {'batch_idx': 295,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 8072, 'row_end': 13071}]},\n",
       " {'batch_idx': 296,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 13072, 'row_end': 18071}]},\n",
       " {'batch_idx': 297,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 72, 'row_start': 18072, 'row_end': 20373},\n",
       "   {'file_idx': 73, 'row_start': 0, 'row_end': 2697}]},\n",
       " {'batch_idx': 298,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 2698, 'row_end': 7697}]},\n",
       " {'batch_idx': 299,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 7698, 'row_end': 12697}]},\n",
       " {'batch_idx': 300,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 12698, 'row_end': 17697}]},\n",
       " {'batch_idx': 301,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 73, 'row_start': 17698, 'row_end': 20373},\n",
       "   {'file_idx': 74, 'row_start': 0, 'row_end': 2323}]},\n",
       " {'batch_idx': 302,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 2324, 'row_end': 7323}]},\n",
       " {'batch_idx': 303,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 5000,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 7324, 'row_end': 12323}]},\n",
       " {'batch_idx': 304,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 4025,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 12324, 'row_end': 16348}]},\n",
       " {'batch_idx': 305,\n",
       "  'target_batch_size': 5000,\n",
       "  'actual_batch_size': 4025,\n",
       "  'segments': [{'file_idx': 74, 'row_start': 16349, 'row_end': 20373}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.batch_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d00bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n",
      "4025\n",
      "4025\n"
     ]
    }
   ],
   "source": [
    "for bm in training_generator.batch_metadata:\n",
    "    print(bm['actual_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:22:52.595653: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 21, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d (QSepar  (None, 11, 19, 5)         33        \n",
      " ableConv2D)                                                     \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 11, 19, 5)         0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 3, 6, 5)           0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 3, 6, 5)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 90)                0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 16)                1456      \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_2 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2029 (7.93 KB)\n",
      "Trainable params: 2029 (7.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((13,21,2),n_filters=5,pool_size=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5132c9ac\n"
     ]
    }
   ],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 01:22:59.737879: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-06-07 01:23:00.097075: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-06-07 01:23:00.946110: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x5594b2114530\n",
      "2025-06-07 01:23:03.831979: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f21fa5bed00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-07 01:23:03.832033: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB MIG 1g.5gb, Compute Capability 8.0\n",
      "2025-06-07 01:23:03.863333: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749252184.117578 1936110 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306/306 [==============================] - ETA: 0s - loss: 17624.6270\n",
      "Epoch 1: val_loss improved from inf to 4556.48340, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.01-t17624.63-v4556.48.hdf5\n",
      "306/306 [==============================] - 39s 96ms/step - loss: 17624.6270 - val_loss: 4556.4834\n",
      "Epoch 2/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: 734.5047\n",
      "Epoch 2: val_loss improved from 4556.48340 to -2782.52783, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.02-t734.50-v-2782.53.hdf5\n",
      "306/306 [==============================] - 27s 87ms/step - loss: 734.5047 - val_loss: -2782.5278\n",
      "Epoch 3/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -4072.5593\n",
      "Epoch 3: val_loss improved from -2782.52783 to -5392.73779, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.03-t-4072.56-v-5392.74.hdf5\n",
      "306/306 [==============================] - 28s 90ms/step - loss: -4072.5593 - val_loss: -5392.7378\n",
      "Epoch 4/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -6116.1001\n",
      "Epoch 4: val_loss improved from -5392.73779 to -7103.79541, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.04-t-6116.10-v-7103.80.hdf5\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -6116.1001 - val_loss: -7103.7954\n",
      "Epoch 5/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -7607.7192\n",
      "Epoch 5: val_loss improved from -7103.79541 to -8419.72754, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.05-t-7607.72-v-8419.73.hdf5\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -7607.7192 - val_loss: -8419.7275\n",
      "Epoch 6/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -7650.1221\n",
      "Epoch 6: val_loss did not improve from -8419.72754\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -7650.1221 - val_loss: -7938.4697\n",
      "Epoch 7/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -8630.1650\n",
      "Epoch 7: val_loss did not improve from -8419.72754\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -8630.1650 - val_loss: -6653.3760\n",
      "Epoch 8/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -9612.1064\n",
      "Epoch 8: val_loss improved from -8419.72754 to -10591.51367, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.08-t-9612.11-v-10591.51.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -9612.1064 - val_loss: -10591.5137\n",
      "Epoch 9/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -10270.4219\n",
      "Epoch 9: val_loss improved from -10591.51367 to -10950.48047, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.09-t-10270.42-v-10950.48.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -10270.4219 - val_loss: -10950.4805\n",
      "Epoch 10/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -10732.3652\n",
      "Epoch 10: val_loss did not improve from -10950.48047\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -10732.3652 - val_loss: -10843.8613\n",
      "Epoch 11/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -11030.0439\n",
      "Epoch 11: val_loss improved from -10950.48047 to -12090.58496, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.11-t-11030.04-v-12090.58.hdf5\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -11030.0439 - val_loss: -12090.5850\n",
      "Epoch 12/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -12171.6260\n",
      "Epoch 12: val_loss improved from -12090.58496 to -12846.22363, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.12-t-12171.63-v-12846.22.hdf5\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -12171.6260 - val_loss: -12846.2236\n",
      "Epoch 13/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -13103.0430\n",
      "Epoch 13: val_loss improved from -12846.22363 to -13924.93359, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.13-t-13103.04-v-13924.93.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -13103.0430 - val_loss: -13924.9336\n",
      "Epoch 14/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -12783.1562\n",
      "Epoch 14: val_loss did not improve from -13924.93359\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -12783.1562 - val_loss: -13677.1230\n",
      "Epoch 15/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14049.2021\n",
      "Epoch 15: val_loss did not improve from -13924.93359\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -14049.2021 - val_loss: -12612.8164\n",
      "Epoch 16/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -13791.6758\n",
      "Epoch 16: val_loss improved from -13924.93359 to -15144.25000, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.16-t-13791.68-v-15144.25.hdf5\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -13791.6758 - val_loss: -15144.2500\n",
      "Epoch 17/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -12970.8340\n",
      "Epoch 17: val_loss did not improve from -15144.25000\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -12970.8340 - val_loss: -14514.1992\n",
      "Epoch 18/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14721.9219\n",
      "Epoch 18: val_loss did not improve from -15144.25000\n",
      "306/306 [==============================] - 26s 84ms/step - loss: -14721.9219 - val_loss: -15119.1221\n",
      "Epoch 19/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14716.9131\n",
      "Epoch 19: val_loss did not improve from -15144.25000\n",
      "306/306 [==============================] - 26s 84ms/step - loss: -14716.9131 - val_loss: -14855.7402\n",
      "Epoch 20/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -13419.4844\n",
      "Epoch 20: val_loss did not improve from -15144.25000\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -13419.4844 - val_loss: -14869.7725\n",
      "Epoch 21/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15068.9639\n",
      "Epoch 21: val_loss improved from -15144.25000 to -15393.91797, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.21-t-15068.96-v-15393.92.hdf5\n",
      "306/306 [==============================] - 28s 91ms/step - loss: -15068.9639 - val_loss: -15393.9180\n",
      "Epoch 22/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15184.1455\n",
      "Epoch 22: val_loss did not improve from -15393.91797\n",
      "306/306 [==============================] - 26s 84ms/step - loss: -15184.1455 - val_loss: -15194.3477\n",
      "Epoch 23/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -13054.4199\n",
      "Epoch 23: val_loss did not improve from -15393.91797\n",
      "306/306 [==============================] - 27s 89ms/step - loss: -13054.4199 - val_loss: -11595.6143\n",
      "Epoch 24/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14577.5488\n",
      "Epoch 24: val_loss did not improve from -15393.91797\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -14577.5488 - val_loss: -15233.0596\n",
      "Epoch 25/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15188.0244\n",
      "Epoch 25: val_loss did not improve from -15393.91797\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -15188.0244 - val_loss: -12916.6328\n",
      "Epoch 26/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14770.5488\n",
      "Epoch 26: val_loss improved from -15393.91797 to -15829.32227, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.26-t-14770.55-v-15829.32.hdf5\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -14770.5488 - val_loss: -15829.3223\n",
      "Epoch 27/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14273.7842\n",
      "Epoch 27: val_loss did not improve from -15829.32227\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -14273.7842 - val_loss: -14160.2168\n",
      "Epoch 28/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15448.1338\n",
      "Epoch 28: val_loss improved from -15829.32227 to -16059.24805, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.28-t-15448.13-v-16059.25.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -15448.1338 - val_loss: -16059.2480\n",
      "Epoch 29/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14945.5781\n",
      "Epoch 29: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -14945.5781 - val_loss: -12396.7354\n",
      "Epoch 30/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14599.6367\n",
      "Epoch 30: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 26s 84ms/step - loss: -14599.6367 - val_loss: -15854.0684\n",
      "Epoch 31/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14376.3516\n",
      "Epoch 31: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -14376.3516 - val_loss: -15125.1758\n",
      "Epoch 32/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14761.6836\n",
      "Epoch 32: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -14761.6836 - val_loss: -15657.6357\n",
      "Epoch 33/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14129.1543\n",
      "Epoch 33: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -14129.1543 - val_loss: -11609.9941\n",
      "Epoch 34/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -14181.1709\n",
      "Epoch 34: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -14181.1709 - val_loss: -15702.9395\n",
      "Epoch 35/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15136.8740\n",
      "Epoch 35: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -15136.8740 - val_loss: -15534.7480\n",
      "Epoch 36/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15434.8496\n",
      "Epoch 36: val_loss did not improve from -16059.24805\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -15434.8496 - val_loss: -15574.5879\n",
      "Epoch 37/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16064.7969\n",
      "Epoch 37: val_loss improved from -16059.24805 to -16942.27539, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.37-t-16064.80-v-16942.28.hdf5\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -16064.7969 - val_loss: -16942.2754\n",
      "Epoch 38/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16385.3945\n",
      "Epoch 38: val_loss did not improve from -16942.27539\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -16385.3945 - val_loss: -16718.0645\n",
      "Epoch 39/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16320.2100\n",
      "Epoch 39: val_loss improved from -16942.27539 to -16961.74805, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.39-t-16320.21-v-16961.75.hdf5\n",
      "306/306 [==============================] - 27s 89ms/step - loss: -16320.2100 - val_loss: -16961.7480\n",
      "Epoch 40/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16093.3867\n",
      "Epoch 40: val_loss did not improve from -16961.74805\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -16093.3867 - val_loss: -15733.6123\n",
      "Epoch 41/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16562.2070\n",
      "Epoch 41: val_loss did not improve from -16961.74805\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -16562.2070 - val_loss: -16414.5020\n",
      "Epoch 42/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16704.3398\n",
      "Epoch 42: val_loss did not improve from -16961.74805\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -16704.3398 - val_loss: -16604.8984\n",
      "Epoch 43/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15353.1865\n",
      "Epoch 43: val_loss improved from -16961.74805 to -17425.45703, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.43-t-15353.19-v-17425.46.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -15353.1865 - val_loss: -17425.4570\n",
      "Epoch 44/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17029.7578\n",
      "Epoch 44: val_loss did not improve from -17425.45703\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -17029.7578 - val_loss: -17293.0117\n",
      "Epoch 45/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16618.8496\n",
      "Epoch 45: val_loss did not improve from -17425.45703\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -16618.8496 - val_loss: -16940.6992\n",
      "Epoch 46/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16528.3789\n",
      "Epoch 46: val_loss improved from -17425.45703 to -17470.03906, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.46-t-16528.38-v-17470.04.hdf5\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -16528.3789 - val_loss: -17470.0391\n",
      "Epoch 47/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16139.0117\n",
      "Epoch 47: val_loss did not improve from -17470.03906\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -16139.0117 - val_loss: -13939.0352\n",
      "Epoch 48/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15727.2197\n",
      "Epoch 48: val_loss did not improve from -17470.03906\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -15727.2197 - val_loss: -16137.1602\n",
      "Epoch 49/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -16548.5000\n",
      "Epoch 49: val_loss improved from -17470.03906 to -17737.92773, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.49-t-16548.50-v-17737.93.hdf5\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -16548.5000 - val_loss: -17737.9277\n",
      "Epoch 50/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17096.7070\n",
      "Epoch 50: val_loss improved from -17737.92773 to -17914.78125, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.50-t-17096.71-v-17914.78.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17096.7070 - val_loss: -17914.7812\n",
      "Epoch 51/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17338.7969\n",
      "Epoch 51: val_loss improved from -17914.78125 to -18225.94727, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.51-t-17338.80-v-18225.95.hdf5\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -17338.7969 - val_loss: -18225.9473\n",
      "Epoch 52/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17455.0898\n",
      "Epoch 52: val_loss did not improve from -18225.94727\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -17455.0898 - val_loss: -12118.6201\n",
      "Epoch 53/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17408.1777\n",
      "Epoch 53: val_loss did not improve from -18225.94727\n",
      "306/306 [==============================] - 27s 89ms/step - loss: -17408.1777 - val_loss: -18170.2637\n",
      "Epoch 54/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17821.9863\n",
      "Epoch 54: val_loss improved from -18225.94727 to -18502.77344, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.54-t-17821.99-v-18502.77.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17821.9863 - val_loss: -18502.7734\n",
      "Epoch 55/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17668.3262\n",
      "Epoch 55: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17668.3262 - val_loss: -18291.6406\n",
      "Epoch 56/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17796.9941\n",
      "Epoch 56: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17796.9941 - val_loss: -18484.1836\n",
      "Epoch 57/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17412.7578\n",
      "Epoch 57: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -17412.7578 - val_loss: -18241.4707\n",
      "Epoch 58/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17358.8555\n",
      "Epoch 58: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -17358.8555 - val_loss: -17642.7656\n",
      "Epoch 59/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17488.5859\n",
      "Epoch 59: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 89ms/step - loss: -17488.5859 - val_loss: -16153.3018\n",
      "Epoch 60/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17687.6445\n",
      "Epoch 60: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17687.6445 - val_loss: -18216.3516\n",
      "Epoch 61/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -15979.0654\n",
      "Epoch 61: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -15979.0654 - val_loss: -17910.5703\n",
      "Epoch 62/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17828.1387\n",
      "Epoch 62: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -17828.1387 - val_loss: -17947.0410\n",
      "Epoch 63/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17514.9141\n",
      "Epoch 63: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -17514.9141 - val_loss: -17508.4883\n",
      "Epoch 64/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17634.9414\n",
      "Epoch 64: val_loss did not improve from -18502.77344\n",
      "306/306 [==============================] - 27s 88ms/step - loss: -17634.9414 - val_loss: -16611.2559\n",
      "Epoch 65/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -18082.0332\n",
      "Epoch 65: val_loss improved from -18502.77344 to -18826.07227, saving model to ./trained_models/model-5132c9ac-checkpoints/weights.65-t-18082.03-v-18826.07.hdf5\n",
      "306/306 [==============================] - 26s 86ms/step - loss: -18082.0332 - val_loss: -18826.0723\n",
      "Epoch 66/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -18322.2383\n",
      "Epoch 66: val_loss did not improve from -18826.07227\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -18322.2383 - val_loss: -18541.2637\n",
      "Epoch 67/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -18423.0664\n",
      "Epoch 67: val_loss did not improve from -18826.07227\n",
      "306/306 [==============================] - 27s 87ms/step - loss: -18423.0664 - val_loss: -17756.7012\n",
      "Epoch 68/1000\n",
      "306/306 [==============================] - ETA: 0s - loss: -17539.9375\n",
      "Epoch 68: val_loss did not improve from -18826.07227\n",
      "306/306 [==============================] - 26s 85ms/step - loss: -17539.9375 - val_loss: -17982.5488\n",
      "Epoch 69/1000\n",
      "115/306 [==========>...................] - ETA: 12s - loss: -18034.0625"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger],\n",
    "        epochs=1000,\n",
    "        shuffle=False,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2706-3e27-4ece-9ebd-be135f6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 1/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: 27173.7227\n",
    "Epoch 1: val_loss improved from inf to 3957.69629, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.01-t27173.72-v3957.70.hdf5\n",
    "305/305 [==============================] - 34s 102ms/step - loss: 27173.7227 - val_loss: 3957.6963\n",
    "Epoch 2/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: 2077.7952\n",
    "Epoch 2: val_loss improved from 3957.69629 to -473.36844, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.02-t2077.80-v-473.37.hdf5\n",
    "305/305 [==============================] - 30s 97ms/step - loss: 2077.7952 - val_loss: -473.3684\n",
    "Epoch 3/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -935.6221\n",
    "Epoch 3: val_loss improved from -473.36844 to -1284.97668, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.03-t-935.62-v-1284.98.hdf5\n",
    "305/305 [==============================] - 61s 199ms/step - loss: -935.6221 - val_loss: -1284.9767\n",
    "Epoch 4/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -484.1406\n",
    "Epoch 4: val_loss improved from -1284.97668 to -1554.20630, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.04-t-484.14-v-1554.21.hdf5\n",
    "305/305 [==============================] - 36s 116ms/step - loss: -484.1406 - val_loss: -1554.2063\n",
    "Epoch 5/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -3135.4961\n",
    "Epoch 5: val_loss improved from -1554.20630 to -2936.62402, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.05-t-3135.50-v-2936.62.hdf5\n",
    "305/305 [==============================] - 40s 130ms/step - loss: -3135.4961 - val_loss: -2936.6240\n",
    "Epoch 6/1000\n",
    " 38/305 [==>...........................] - ETA: 19s - loss: -3885.3333\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
