{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ec3747-31c9-451d-b06b-fe4e589ed725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e30cc2-2e02-447e-a79e-189d71bf188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove TF warnings (this can be dangerous)\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23456d02-c5e1-4f81-b8f3-c8c5367dc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://gist.github.com/zrruziev/b93e1292bf2ee39284f834ec7397ee9f\n",
    "# sudo echo 0 | sudo tee -a /sys/bus/pci/devices/0000\\:01\\:00.0/numa_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from qkeras import *\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import psutil\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da258a7-b1d2-4afe-aa30-2ddb31150ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# You can disable the GPU, if a GPU is present\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from OptimizedDataGenerator import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb31997-6ba4-4af7-8397-9b28e315e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model type\n",
    "model_type = \"xysum\" \n",
    "\n",
    "# True = use all of the time slices, False = use a subset of the timeslices\n",
    "timeslices_all_enable = False\n",
    "\n",
    "# True = load tfrecords, False = (re-)generated tfrecords\n",
    "load_from_tfrecords_enabled = True\n",
    "\n",
    "# True = load model from file, False = train it from scratch\n",
    "load_model_from_hdf5_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9199b73a-4d33-48aa-8d2c-a7f5b5376496",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"xysum\":\n",
    "    from xysum_model import *\n",
    "else:\n",
    "    assert False, \"Selected model cannot be found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7d7663-05e8-4790-84dd-11c25e41f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use config info from file: /home/giuseppe/research/projects/smartpixels/data/dataset_2s, /home/giuseppe/research/projects/smartpixels/davidgjiang-smart-pixels-ml/tfrecords, /home/giuseppe/research/projects/smartpixels/davidgjiang-smart-pixels-ml/npy, /home/giuseppe/research/projects/smartpixels/davidgjiang-smart-pixels-ml/weights\n"
     ]
    }
   ],
   "source": [
    "# You can define a JSON configuration file locally\n",
    "# {\n",
    "#    \"data_base_dir\": \"/data/dajiang/smartPixels\",\n",
    "#    \"tfrecords_base_dir\" : \"/data/dajiang/smartPixels\",\n",
    "#    \"model_base_dir\": \"/home/dajiang/smart-pixels-ml/weights\"\n",
    "# }\n",
    "config_file_path = 'config.json'\n",
    "\n",
    "# If the file does not exist, the notebook uses default values for those entries\n",
    "data_base_dir = \"/data/dajiang/smartPixels/dataset_2s\"\n",
    "tfrecords_base_dir = \"/data/dajiang/smartPixels/tfrecords\"\n",
    "npy_base_dir = \"/data/dajiang/smartPixels/npy\"\n",
    "model_base_dir = \"/home/dajiang/smart-pixels-ml/weights\"\n",
    "\n",
    "if os.path.exists(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data_base_dir = data.get('data_base_dir')\n",
    "        tfrecords_base_dir = data.get('tfrecords_base_dir')\n",
    "        npy_base_dir = data.get('npy_base_dir')\n",
    "        model_base_dir = data.get('model_base_dir')\n",
    "    print(f\"Use config info from file: {data_base_dir}, {tfrecords_base_dir}, {npy_base_dir}, {model_base_dir}\")\n",
    "else:\n",
    "    print(f\"File does not exist. Use default config info: {data_base_dir}, {tfrecords_base_dir}, {npy_base_dir}, {model_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e1a0184-55a3-4294-b0df-b3067d9bb72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 4.46 ms, total: 167 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 50\n",
    "val_file_size = 10\n",
    "\n",
    "# See: https://docs.google.com/document/d/1ZoqVyJOOAXhzt2egMWh3OoNJ6lWq5lNR6sjcYON4Vlo/edit?tab=t.0#heading=h.k6tyal7z5t5l\n",
    "dataset_name = \"dataset_2s\"\n",
    "# 50x12.5x100 micron pixel sensor => 13x21 pixel sensor array\n",
    "sensor_geometry_name = \"50x12P5x100\"\n",
    "# Either 20 or 2 timeslices\n",
    "timeslices_name = \"timeslices20\" if timeslices_all_enable else \"timeslices2\"\n",
    "timeslices_range = -1 if timeslices_all_enable else [0, 19]\n",
    "timeslices_val = 20 if timeslices_all_enable else 2\n",
    "#\n",
    "batch_size_name = f\"bs{batch_size}\"\n",
    "\n",
    "# Input: parquets\n",
    "data_dir = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/recon3D/\"\n",
    "labels_dir = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/labels/\"\n",
    "\n",
    "# Output: tfrecords\n",
    "tfrecords_dir_train = f\"{tfrecords_base_dir}/tfrecords_{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}_train\"\n",
    "tfrecords_dir_val = f\"{tfrecords_base_dir}/tfrecords_{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}_val\"\n",
    "\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    data_directory_path = data_dir,\n",
    "    labels_directory_path = labels_dir,\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    scaling_list = [75.0, 18.75, 10.0, 1.22],\n",
    "    input_shape = (timeslices_val,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    files_from_end = True,\n",
    "    shuffle = True,\n",
    "\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train if load_from_tfrecords_enabled else None,\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = timeslices_range,\n",
    "    max_workers = 1, # Don't make this too large (will use up all RAM)\n",
    "    seed = 10,\n",
    "    quantize = True # Quantization ON\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    data_directory_path = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/recon3D/\",\n",
    "    labels_directory_path = f\"{data_base_dir}/dataset_2s_50x12P5_parquets/unflipped/labels/\",\n",
    "    is_directory_recursive = False,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    scaling_list = [75.0, 18.75, 10.0, 1.22],\n",
    "    input_shape = (timeslices_val,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    files_from_end = True,\n",
    "    shuffle = True,\n",
    "\n",
    "    load_from_tfrecords_dir = tfrecords_dir_val if load_from_tfrecords_enabled else None,\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = timeslices_range,\n",
    "    max_workers = 1, # Don't make this too large (will use up all RAM)\n",
    "    seed = 10,\n",
    "    quantize = True # Quantization ON\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd324b5a-9a82-4b57-9de0-94ba5e76715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/research/projects/smartpixels/davidgjiang-smart-pixels-ml/npy/y_timeslices2_val.npy\n",
      "(488705, 13, 21, 2)\n",
      "(488705, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array that contains the recon3D and labels information\n",
    "# X is the 20-timeslices or 2-timeslices recon3D data\n",
    "# y is the labels data ['x-midplane','y-midplane','cotAlpha','cotBeta']\n",
    "X_val_all = []\n",
    "y_val_all = []\n",
    "\n",
    "num_batches = validation_generator.__len__() # The total number of batches for the validation dataset\n",
    "#val_num_batches = 1\n",
    "\n",
    "for i_batch in range(num_batches): # Loop over all batches\n",
    "    X_val, y_val = validation_generator.__getitem__(i_batch)\n",
    "    X_val = X_val.numpy()\n",
    "    y_val = y_val.numpy()\n",
    "    X_val_all.append(X_val)\n",
    "    y_val_all.append(y_val)\n",
    "\n",
    "X_val_all = np.array(np.concatenate(X_val_all))\n",
    "y_val_all = np.array(np.concatenate(y_val_all))\n",
    "\n",
    "os.makedirs(npy_base_dir, exist_ok=True)\n",
    "np.save(f\"{npy_base_dir}/X_{timeslices_name}_val.npy\", X_val_all)\n",
    "np.save(f\"{npy_base_dir}/y_{timeslices_name}_val.npy\", y_val_all)\n",
    "\n",
    "print(f\"{npy_base_dir}/y_{timeslices_name}_val.npy\")\n",
    "print(X_val_all.shape)\n",
    "print(y_val_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9fd3960-c7f1-4d6c-b7e0-0c4c833797d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [13, 1, 2], output_shape = [13, 20]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 13, 1, 2), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compiles model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCreateModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtimeslices_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m/extras2/home/gdg/research/projects/smartpixels/lindsey-smart-pixels-ml/xysum_model.py:81\u001b[0m, in \u001b[0;36mCreateModel\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCreateModel\u001b[39m(shape):\n\u001b[1;32m     80\u001b[0m     x_base \u001b[38;5;241m=\u001b[39m x_in \u001b[38;5;241m=\u001b[39m Input(shape)\n\u001b[0;32m---> 81\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[43mconv_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#stack = AveragePooling2D(\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m#    pool_size=(2, 2), \u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m#    strides=None, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#)(stack)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m#stack = QActivation(\"quantized_bits(8, 0, alpha=1)\")(stack)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     stack \u001b[38;5;241m=\u001b[39m var_network(stack, hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n",
      "File \u001b[0;32m/extras2/home/gdg/research/projects/smartpixels/lindsey-smart-pixels-ml/xysum_model.py:42\u001b[0m, in \u001b[0;36mconv_network\u001b[0;34m(var, kernel_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconv_network\u001b[39m(var, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     36\u001b[0m     proj_x \u001b[38;5;241m=\u001b[39m AveragePooling2D(\n\u001b[1;32m     37\u001b[0m         pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m), \n\u001b[1;32m     38\u001b[0m         strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     39\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     40\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,        \n\u001b[1;32m     41\u001b[0m     )(var)\n\u001b[0;32m---> 42\u001b[0m     proj_x \u001b[38;5;241m=\u001b[39m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     proj_y \u001b[38;5;241m=\u001b[39m AveragePooling2D(\n\u001b[1;32m     44\u001b[0m         pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m     45\u001b[0m         strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     46\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     47\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,        \n\u001b[1;32m     48\u001b[0m     )(var)\n\u001b[1;32m     49\u001b[0m     proj_y \u001b[38;5;241m=\u001b[39m Reshape((\u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m20\u001b[39m))(proj_y)\n",
      "File \u001b[0;32m~/ccs_venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/ccs_venv/lib/python3.10/site-packages/keras/src/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [13, 1, 2], output_shape = [13, 20]\n\nCall arguments received by layer \"reshape\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 13, 1, 2), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# compiles model\n",
    "model = CreateModel((13,21,timeslices_val))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b8741-98fd-4e1e-bc29-7b9fcbb9b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_name = f\"{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}\" + ((\"_\" if model_type != \"\" else \"\") + model_type)\n",
    "\n",
    "best_model_hdf5 = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}.hdf5\"\n",
    "best_model_keras = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}.keras\"\n",
    "best_model_weights_hdf5 = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_weights.hdf5\"\n",
    "best_model_weights_keras = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_weights.keras\"\n",
    "best_model_architecture_json = f\"{model_base_dir}/weights_7pitches/best_model_{model_name}_architecture.json\"\n",
    "\n",
    "print(f'Model name: {model_name}')\n",
    "\n",
    "if not load_model_from_hdf5_enabled:\n",
    "    # training\n",
    "    es = EarlyStopping(\n",
    "        patience=50,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    checkpoint_base_dir = f\"{model_base_dir}/weights_7pitches/{dataset_name}_{sensor_geometry_name}_{timeslices_name}_{batch_size_name}\"  + (((\"_\" if model_type != \"\" else \"\") + model_type)) + \"-checkpoints\"\n",
    "\n",
    "    os.makedirs(checkpoint_base_dir, exist_ok=True)\n",
    "    checkpoint_filepath = checkpoint_base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'\n",
    "    mcp = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=False,\n",
    "    )\n",
    "\n",
    "    class ScalePrintingCallback(keras.callbacks.Callback):    \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            scale_layer = self.model.layers[-1]\n",
    "            print(\n",
    "                f\"scaling layer ({epoch}):\", \n",
    "                scale_layer.scale, \n",
    "                tf.math.softplus(scale_layer.scale)\n",
    "            )\n",
    "\n",
    "    print_scale = ScalePrintingCallback()\n",
    "    \n",
    "    history = model.fit(x=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        callbacks=[mcp],\n",
    "                        epochs=100,\n",
    "                        shuffle=False, # shuffling now occurs within the data-loader\n",
    "                        verbose=1)\n",
    "\n",
    "    # Revert to best model\n",
    "    files = os.listdir(checkpoint_base_dir)\n",
    "    vlosses = [float(f.split(\"-v\")[1].split(\".hdf5\")[0]) for f in files]\n",
    "    bestfile = files[np.argmin(vlosses)]\n",
    "    model.load_weights(f\"{checkpoint_base_dir}/{bestfile}\")\n",
    "\n",
    "    # Save (best) model information to file\n",
    "    model.save(best_model_hdf5)\n",
    "    model.save(best_model_keras)\n",
    "    model.save_weights(best_model_weights_hdf5)\n",
    "    model.save_weights(best_model_weights_keras)\n",
    "    model_json = model.to_json()\n",
    "    with open(best_model_architecture_json, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "else:\n",
    "    co = {\"custom_loss\": custom_loss}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    # This overrides the previously compiled model\n",
    "    # TODO: load just weights\n",
    "    model = load_model(best_model_hdf5, custom_objects=co)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd172d9c-ac6d-497a-8391-dc507cb3cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_loss_png = f\"{model_base_dir}/weights_7pitches/training_validation_loss_{model_name}.png\"\n",
    "if load_model_from_hdf5_enabled:\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    img = Image.open(training_validation_loss_png)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else: \n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(training_validation_loss_png, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17336580-88ed-48ca-ad44-540c0ce474df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
